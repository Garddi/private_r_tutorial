---
title: "R Tutorial"
author: "Gard Olav Dietrichson"
format: html
execute: 
  warning: false
toc: true
toc-location: right
number-sections: true
theme:
  light: flatly
  dark: darkly
---

# Introduction

Welcome to a (more or less) thorough walkthrough of R in applied areas. Covers the sections: General Structure, Data import, Simple Data checking, Data Wrangling, Plotting, and advanced R methods. 

# General structure

## Data Types

Information takes several forms in R, all attached with a class, the basic forms of data that needs to be known are numeric (will appear as "double" sometimes, this is related to memory allocation, but just means more decimals numeric), integer (whole numbers), factor (text but with numbers assigned to certain values), character, boolean (logical, true or false), and several types of Date data. Additionally there are a class of missing values, but they are all covered by NA.

```{r}
1 + 1 ### Comments are in hashtags 

"hello" ## Strings or characters

NA

## NA can also have type specific values, but these are *never* used

NA_character_
NA_complex_ ## All yield same output

## Boolean datas

TRUE
FALSE

```
All entries in a script (which is just a text file), are sent to the console which interprets the text. There are also certain special datatypes, such as NULL for empty (which is not the same as missing), Inf for infinite values (dividing by zero for instance), and NaN (Not a Number, errors in certain computational processes)

## Assignment

While strictly speaking a functional programming language, object oriented programming is highly supported, and the norm for usage amongst data scientists. Most outputs in the language will be assigned a name, and appear in the global environment (in one of the other windows in your RStudio IDE). This is done either with a `=` or (more commonly, due to specific interactions with an equals sign) with the `<-` known as the assignment operator.  

```{r}
hello <- 23

hello ## Calling the object prints the content by default
```


### Other operators

Other operators become important when discussing logical tests or other mathematical operations. 

- +, -, /, *, ^ all does what you would expect
- == Strict logical test, is this equal to the other thing, response in boolean
- != Strict anti-logical test, is this thing different to the other thing, response in boolean
- <, >, <=, >=, less than, larger than, less or equal than, larger or equal than, logical tests, returns boolean
- &, |, Binding operators for more logical tests & (AND operator) requires both elements to be true, while | (OR operator) requires only one to be true
- %in% Contained logical test, is there a unit in this other element which is strictly equal to the first thing.
- ! in general reverses the proceeding logical test, so if you write `!("YES" == "YES")` it will ask if the thing inside the parenthesis _is not_ the case, reversing the logical test. 

```{r}
"Yes" == "Yes"

"Yes" == "No"

"Yes" != "Yes"

"Yes" != "No"

"Yes" %in% c("No", "no", "Maybe", "Yes")

```

Logical tests are also relevant through certain functions, these functions all return either TRUE or FALSE values, which is useful in several cases, inside other functions or for flow control in loops (all of this is covered later). These functions include `is.na`, `is.empty`, `is.character`, `is.numeric`, etc. Lots of functions to explore that start with `is.`

## Vectors

All complex data of interest to us in R is either a vector, a list or a dataframe (Matrix and Arrays exist, but are not relevant to this scope). Vectors are uni-dimensional with strict same type requirement, dataframes are two-dimensional, and lists are uni-dimensionals with no requirement for consistency among entries.

```{r}
# Vector
a_vector <- c(1, 3, 5, 6, 7,1)

```
All dimensional datatypes can be indexed using brackets, to call individual elements of the object

```{r}
a_vector[4] ## Fourth number of the vector above
```
## Dataframes

Two dimensional data structures are referred to as dataframes, and consists of rows of units across columns of variables. Columns are essentially vectors, and must therefore be of the same type. Indexing is done using brackets, and has to be separated with a comma, first number is the rownumber, second is the column number. Empty spaces extracts the whole row/column.

```{r}
df_example <- data.frame(names = c("Gard", "Grad", "Gard Olav", "the Notorious G.A.R.D."),
                         age = c(27, 23, 21, 19),
                         fav_movie = c("The Adolescence of Utena", "12 Angry Men", "Inception", "Straight outta Compton"),
                         happy = c(FALSE, TRUE, FALSE, TRUE))

df_example[1,]

df_example[c(1,2),]

df_example[,1]

df_example[,1:2]

df_example[3,2]

## Selecting on names

df_example[,c("fav_movie")]

## Index tool

df_example$age

df_example$happy

```

The most common process of accessing variables when using dataframe objects however, is to use the $ sign, which is an indexation tool for column names.

## Lists

Lists can have anything in them, even dataframes or vectors, with no rules as to consistency over elements. Elements are then stored unidimensionally, accessed with double brackets for a relevant element. 

```{r}
ex_list <- list(df_example, a_vector)

ex_list[[2]]

```
Lists are not often used in our branch of datascience, but will appear as accidental outputs to certain functions and processes, so its good to know what they are. 

## Functions

Functions are, the single most important aspect that exists in R, they are not commands, this ain't stata. Understanding the universe of functions afforded to you through a multitude of packages is the key to conducting efficient data processing in R. 

A function has the general form of `function_name(argument_1, argument_2, ... argument_n)`. All functions have clearly defined arguments, which control the outcome of the function. What these arguments are, are specified in the help file, which can be called upon by typing `?function` in the console. Sometimes these arguments are data oriented, the first argument in many tidyverse functions for instance, is a dataframe. Others are control oriented, meaning that they control specifications of the function itself. The function `mean()` for instance, takes three primary arguments, one is a vector of numbers (it also strictly only accepts dataypes that can be interpreted as numbers, so no character means), the second is a trim value, which is set to trim certain percentages of each side of the numerical distribution. Finally there is a na.rm argument, which controls whether or not to remove missing values before computing, if not done, a vector with missing values will return an error from the function. 

```{r}
mean(a_vector)

mean(a_vector, trim = .2)

new_vector <- c(1,3,5,6,7,3,NA)

mean(new_vector)

mean(new_vector, na.rm = TRUE)
```


# Simple Analysis

Before you can use R to conduct data wrangling or very directed plotting, you should utilize some functions to explore the data you are interested in. In this section I will utilize a singular dataset to demonstrate some of the simpler data exploring techniques. 

First thing to address is data import. Based on the type of data applied, you utilize a particular function, usually found in the `haven` package, or just in base R. The most common datatypes are .csv for comma separated files, .xlsx files for excel (lots of strange things can happen when reading these files, be mindful), .sav for stata files, additionally, R has a set of unique filetypes that are relevant, .rda, .rds, and .rdata, .rda and .rdata are loaded with a simple `load` function, while .rds is read using a `readRDS` function.  Each has an appropriate function for reading the relevant data. I load in a compiled set from WVS from South Korea and Japan here.

```{r}

## Note the file path specification is also very specific. Two dots signal
## a directory above your current. So this data is found in 
## the parallell folder MA thesis, which is parallell to the folder two layers
## above the current directory, then with a specified path to find it.

library(haven)

load("../../MA thesis/Data and R scripts/Survey Data/SurvData.Rdata")



```


## Simple data structure functions

For simple sakes, we can explore the structure of the dataset through just clicking on it in the global environment, the same effect can be achieved by using the `view` function, either in the console or in the script (but it can get annoying when re-running it inside the script). For a more systematized view of the data, the `head`, `str`, and `summary` functions are often used. Try them on your own data, but I will utilize head here, because it looks the best in this type of document.

```{r}
head(SurvData)
```

This dataset is clearly already been processed a bit, since a WVS would probably have far more variables. But for now we can tell that there are a set of variables that could interest us. I often recheck the precise names by calling the `names` function, which just prints out the names of the columns in the dataset. When specifying columns in tidyverse functions this can be a helpful reminder.

```{r}
names(SurvData)
```
This is all on an overview level, to give you an idea of what the data looks like and how to further explore it. When it comes to familiarizing yourself with aspects of the dataset, the $ sign for indexing columns becomes helpful. I usually pair this with a `table` function, or a `summary` function, which gives me an idea of what the distribution looks like. For simple interaction exploration, `table` also accepts several variables so you can see where they overlap

```{r}
summary(SurvData$gender)

table(SurvData$gender)

table(SurvData$gender, SurvData$whostility)
```
Another key function to use both when exploring your data, but also when you are troubleshooting issues later, is the `class` function. Quite simply it tells you what type of data the thing you are entering is. It works both on vectors/variables, which can let you know if you are dealing with a character variable or a numeric one, but also on complex objects in your environment, so you can know whether the object you are using is appropriate for certain functions. For instance, map visualisations usually requires objects of the `sf` class, not just dataframes. 

```{r}
class(SurvData$gender)

class(SurvData$country)

class(df_example$happy)

class(SurvData)


```

Furthermore, you can use some very simple plots to help you view the distribution, such as a density plot, or a histogram for certain values.

```{r}
plot(density(SurvData$whostility))

hist(SurvData$whostility)
```
Additionally, some dataforms have labels attached to their columns, this is normal for data imported from spss, or stata. These labels provide information about the coding, without searching the codebook, these can be found with the `attributes` function. I load a dataset here, which has this type of labels. This data will also be used later.

```{r}
load("../../advanced_statistics_phd/my_data/simple_set.rda")

attributes(joined_sets2$straff)

```

 It is also in Norwegian, apologies for that. 

# Data wrangling

Transforming data is best done through the tidyverse system. The most key aspect of this framework is the usage of the pipe operator `%>%`, which takes the output from whatever is in front of the pipe, and gives it as the first argument in the function following the pipe (As a sidenote, if you want to call the preceeding object again, you can use a . to put it more places). Tidyverse allows for many useful simplifications, such as selecting columns by names without complex indexing or name wrapping. Selecting in tidyverse also supports something called tidyselect functions, which allows you to select certain variables based on a pattern, such as the `starts_with` function, which selects all variables that start with a specified string pattern. If you have a vector of your variable names you can also use the `all_of` function to select all of thoes. In this section I will use the dataset previously outlined in the section on simple analysis. I will go through the most common data wrangling functions that I use in my processes.

```{r}
library(tidyverse)

## A simple pipe explanation

SurvData %>% 
  pull(whostility) %>% 
  mean(na.rm = TRUE)

# Is the same as 

mean(pull(SurvData, whostility), na.rm = TRUE)

```


## Mutating

Mutate is the dplyr function for changing variables in a dataset. This means that the function can both create and modify existing variables in the dataset. As we saw in the last structure of our dataset, the gender variable is operationalized as 1 for men and 2 for women, we usually prefer a binary for this, so let us change that variable in a new dataset. Note that variable selecting in tidyverse functions are often quite straightforward, most if not all functions can specify variables by just writing them out in plain text as seen below. 

```{r}
SurvData <- SurvData %>% 
  mutate(gender_subtracted = gender - 1)

## Lets compare the outputs to ensure successful transformation

table(SurvData$gender, SurvData$gender_subtracted)

table(SurvData$country)


```
A common thing you might find with your dataset is that the variables have the wrong type. Perhaps a column that was meant to be a number was read as a character, because the idiot who made the excel sheet decided to spell out "no data" in the missing cells (these people should be executed). In many instances the simplest way of changing a variable, is to force it into another class by using an `as.` function, `as.numeric`, `as.character`, `as.Date`, and `as.double` are common variations of this. There is also the `factor` function, which allows you to create ordinal values. Where the `as.` functions are relatively simple (perhaps except the date one), factor also requires that you supply a vector of the rankings of the factor. 

```{r}

SurvData <- SurvData %>% 
  mutate(gender_text = as.character(gender))

class(SurvData$gender_text)

head(SurvData$gender_text)

```



### Ifelse and Case when mutations

One of the most common techniques for assigning new values, other than just committing straight mathematical operations on them, is to utilize a logical test to assign new values. The most simple way of doing this is the `ifelse` function. It, simply put, tests each element of a vector and then assigns a new value based on TRUE or FALSE response, the syntax is as follows `ifelse(logicaltest, value_if_TRUE, value_if_FALSE)`. If for instance we wanted the new gender variable to be a character based on gender we could do the following.

```{r}
SurvData <- SurvData %>% 
  mutate(gender_ifelsed = ifelse(gender == 2, "Woman", "Man"))

table(SurvData$gender, SurvData$gender_ifelsed)

```
This is doable for instances where the new variable is dichotomous, and while it is possible to put a second `ifelse` function inside the response for the results of the logical test, this can get extremely cluttered as several cases appear. Which is why the dplyr package, helpfully has the `case_when` function, which is a vectorized version of ifelse, the helpfile will even say so. `case_when` has a bit more complex of a syntax, where the logical test is followed by a ~ and then the assigned value, with commas separating all cases. 

```{r}
SurvData <- SurvData %>% 
  mutate(gender_cased = case_when(  ## Style guide recomends this style
    gender == 2 ~ "Woman",
    gender == 1 ~ "Man",
    gender == 3 ~ "Non-Binary"
  ))

table(SurvData$gender, SurvData$gender_cased)

```


### Across

If you have a long set of variables you want to do the same thing to, the `across` selection function can be very helpful. When used within a mutate function it can apply the same function unto several columns. The function assignment is signalled here with a ~ which is called a "purr-style" lambda function, but other ways of assigning the function is technically possible. The .x following signifies the place the column to change has in the function used in it.  

Across can also be extended with anonymous functions (that is technically what is being done behind the scenes), which I will cover later in the document. 

```{r}

vars_i <- names(joined_sets2)[16:29]

joined_sets2_mutated <- joined_sets2 %>% 
  mutate(across(all_of(vars_i), ~ haven::zap_labels(.x))) %>% ## Heres a fun interaction with haven...
  mutate(across(all_of(vars_i), ~ ifelse(.x > 2.5, "High", "Low")))


head(joined_sets2[,16:29])
#Compared with
head(joined_sets2_mutated[,16:29])


```



## Filtering

The second common thing to do when treating data is to filter it, keeping only certain observations. Again you would use a logical test in order to filter these things, so get used to writing these sorts of tests. I showed earlier that the data had two countries, lets make a new dataframe with only Japan. Note also that this function will remove lines, and as such, you should not overwrite the old dataframe when filtering, like I did in the mutate section.

```{r}
jap_surv <- SurvData %>% 
  filter(country == "Japan")

```

We can add more tests to filter it further, either with a binding logical operator, or a comma, but comma adds strict additional requirement. 

```{r}
jap_surv_w <- SurvData %>% 
  filter(country == "Japan", gender_ifelsed == "Woman")

table(jap_surv_w$country, jap_surv_w$gender_ifelsed)

women_or_japanese <- SurvData %>% 
  filter(country == "Japan" | gender_ifelsed == "Woman")

table(women_or_japanese$country, women_or_japanese$gender_ifelsed)

```



## Grouping and Aggregating

A common need is also to use lower level data to create data on summaries of aggregate level, like the average level of women respondents across the survey years. This also has a slightly complex syntax, but you first group the dataframe with the function `group_by`, which specifies a set of variables that units should be grouped into, and then feed that into the `summarise` (`summarize` works too for americans) function, which specifies the new variables you want to create on the aggregate level. These new variables need to be specified by functions that summarise elements however, such as `mean`. In this code I will make an aggregate of Japan and South Korea over the time periods, with the number of respondents in each, and the average degree of anti-women legislator sentiment.

```{r}
grouped_f <- SurvData %>% 
  mutate(whostility = ifelse(whostility < 0, NA, whostility)) %>%  # You can add the unchanged vector as a responsein an ifelse, Values less than 0 are missing categories in WVS
  group_by(country, year) %>% 
  summarise(respondents = n(),
            whostility = mean(whostility, na.rm = TRUE))


head(grouped_f)


```
Given that grouping is part of tidyverse, we can also use tidyselect within it. Lets say I want the mean of several variables.

```{r}

my_vars <- c("PolDisc", "interest", "whostility")

grouped_more <- SurvData %>% 
  mutate(across(all_of(my_vars), ~ ifelse(.x < 0, NA, .x))) %>% 
  group_by(country, year) %>% 
  summarise(across(all_of(my_vars), ~ mean(.x, na.rm = TRUE)))


head(grouped_more)

```
Here we see a NaN example, where a mean of nothing is not a number, since we specified na.rm to be true. 



## Joining 

The most common iteration of joining is the `left_join` function. This joins a secondary dataset to the right of the first dataset. In order to join the datasets you must specify a set of "keys", a set of variables which the values in the left dataset is supposed to match to the right dataset. These do not have to create an exact match between datasets, multiple points in the original set can match single values in the joining set, but the reverse makes things complicated. If a single row in the original set matches multiple rows of the new one, then duplicates are created. For this reason make sure that the original dataset is either on a sub-level hierarchically, or that there are unique and exact matches in the new dataset. 

The most common scenario to join values in my experience is when you have aggregate data that you want to join in to individual level data. Say I have survey result data from certain countries, and I want to join in some contextual variables. I could match that on just country and year variables. Below I utilize the survey in Norwegian parties, first I aggregate a frame, then join it. 

```{r}
joined_lim <- joined_sets2 %>% 
  mutate(eu = ifelse(eu %in% c(1,2,3,4,5), eu, NA)) %>% 
  select(kjonn, parti, tid, eu)

joined_lim_agg <- joined_sets2 %>% 
  mutate(eu = ifelse(eu %in% c(1,2,3,4,5), eu, NA)) %>% 
  group_by(parti, tid) %>% 
  summarise(mean_eu = mean(eu, na.rm = TRUE))

joined_lim[c(3,4,64,65,556,557,1231,1232,5688, 5689,6783, 6784),] ## Indexing some random rows, so we can see variance

joined_lim_joined <- joined_lim %>% 
  left_join(joined_lim_agg, by = c("parti", "tid")) # Here i join in the new set, i know that the parti and tid variables are in both sets

joined_lim_joined[c(3,4,64,65,556,557,1231,1232,5688, 5689,6783, 6784),] ## Indexing the same numbers
```
Since `mean_eu` was the only variable in the aggregated set except the key variables `parti` and `tid`, this is the only new column in the merged dataset. 

`left_join` is by far the most common way of joining a dataset when you want new information or elements to be added to your existing dataset. There also exist `right_join` and `full_join`, but `right_join` is practically the same as left, just less intuitive and more niche in my opinion, while `full_join` should not be used because I can rarely think of a scenario where it makes sense to use in relation to dataset specifications. 

The other relevant form of merging is when you have new data that is identical in structure to the old data, but they represent perhaps new observations to that data. In this instance you seek to bind the rows together, and suitably the function is just called `bind_rows`, if the datasets are structured properly, then you can just enter two datasets in that function and it should merge them. If they are not similar, you have to data wrangle them a bit. 

## Pivoting

Say we have a dataset where differences in years are stored in columns, and we want a single row for each yearly observation. Or we have data that is more extensive than we want, and we want to limit the number of rows. This can be achieved through pivot functions, primarily `pivot_wider` and `pivot_longer` which have fairly intuitive names, they make the dataframe wider or longer, by making it shorter or narrower, respectively. The way to use them is to feed a dataframe into it, with a pipe of course, and then, depending on wider or narrower, you have to specify which columns the names go to or from, and where the values go to and from. For a wider pivot, the new names are all a class of variables, such as a character string for instance, when specifying `names_from`, this is the variable that the new columns will be named after. 

Lets use a different dataset for a more applicable situation. I am here loading a dataset from a survey which is meant to measure agreement within party structures, I transform the question values to instead be distance to the mean of the party. 


```{r}

## Will limit this dataset to only a single party, and a couple of questions and key variables
## Ignore the complex code, it might make sense later

vars_i <- names(joined_sets2)[16:29]

mean_values_for_transf <- joined_sets2 %>% 
  select(parti, tid, all_of(vars_i)) %>% 
  group_by(parti, tid) %>% 
  summarise(across(all_of(vars_i), ~ mean(.x, na.rm = TRUE))) %>% 
  pivot_longer(cols = all_of(vars_i), names_to = "question", values_to = "mean_vals")


head(mean_values_for_transf)


```

Note that we now only have four variables, but far more rows, instead of each question being a variable, each question is now a row for an individual. Lets do the same for the un-summarised version, then `left_join` this aggregate. 

```{r}
joined_sets2_i <-joined_sets2
joined_sets2_i$id <- 1:nrow(joined_sets2) ## If you want a fun story, ask why this is here........


new_joined <- joined_sets2_i %>% 
  mutate(across(all_of(vars_i), ~ haven::zap_labels(.x)),
         across(all_of(vars_i), ~ ifelse(!(.x %in% c(1,2,3,4,5)), NA, .x))) %>%  ## Likert scale, so outside 1-5 is missing vals
  pivot_longer(cols = all_of(vars_i), names_to = "question", values_to = "q_response") %>% 
  left_join(mean_values_for_transf, by = c("parti", "tid", "question")) ## Note the join so that the mean vals for the questions is there as well

dim(new_joined) ## Gets the dimensions of the object

```
As you can see the new frame has 136010 rows, and 23 columns, longer and narrower than the original one. Let's now turn that frame back to the original set, but now with "distance from party-year mean" as the variable on each question. Note that `pivot_longer` requires you to specify the columns that should be pivoted from, in addition to their destination, while `pivot_wider` knows that the names are going to be column names, and the values fill columns. That was poorly worded, remind me to come back to that and make it make sense.

As a sidenote, this section had to go through several revisions, because nothing works on first attempt, so do not be fooled into thinking this is an easy process, but practice makes perfect, and by using this language for yourself you eventually become quite apt at finding the source of your mistakes (and very good at googling). 

```{r}

dist_means_joined <- new_joined %>% 
  mutate(dist_score = q_response - mean_vals) %>% 
  select(-q_response, -mean_vals) %>% ### Removing these because they are reduntant and would create strange frames
  pivot_wider(names_from = "question", values_from = "dist_score")

head(joined_sets2[,vars_i])

head(dist_means_joined[,vars_i])

```



# Plotting

To paraphrase the Beatles, all you need is ggplot. This framework allows you to plot essentially everything, although some packages obviously makes this plotting simpler, but most of them are just wrappers for ggplot code (even some python and Julia packages use ggplot in R as a backend). You could write entire books worth of information on plotting with ggplot. In fact someone did, I have it if you're interested. 

![neat book](./images/healy_vis.jpeg)

## Layers

It is prudent to think of visualisations in ggplot as layered. These layers are bound together with plus signs, and each construct a layer ontop of the plot or other visualisation you want to make. The most basic general layer that all ggplots need is the base layer of the `ggplot` function itself. In this function you specify your dataset, and the general aesthetics. Within the `ggplot` function (and most `geom_` derivatives) there is an `aes` argument, this is an aesthetic argument, it specifies what variable from the dataset should represent what element of the plot. Based on the plot type the type of aesthetic accepted varies, but a list of potential aesthetics are: x, y, color, fill, shape, size, linewidth, linetype, etc. Following this, you add a layer that specifies what type of plot you want, this is done through a family of `geom_` functions such as `geom_point` for scatterplot, `geom_bar` for a barplot, `geom_boxplot` for a boxplot with distributions, etc. Which you use depends on how you want to visualise. If you are curious about the options, type `geom_` in R, and switch through the optins that it auto-completes to. Lets use this information to build a scatterplot of the first grouped dataframe created earlier.

```{r}
grouped_f %>% ### Obviously you can pipe it in 
  ggplot(aes(x=year, y=whostility, color = country)) + 
  geom_point()

```
 Very ugly, but it does the job. We can now add a couple of layers to improve the look. There are several themes that can be used by default, such as `theme_bw` which is my favourite, just add the function in a layer. Additionally we can modify the labels of the plot through the `lab` function as a separate layer. 
 
```{r}
grouped_f %>% 
  ggplot(aes(x=year, y=whostility, color = country)) + 
  geom_point() + 
  theme_bw() + 
  labs(title = "Here's a Title", x = "Time period", y = "Anti-women sentiment", color = "Region")
```
 For the final plot, I will also do some changes to the datastructure so that it visualises a bit better, there's perhaps too much space between the timings as it is. Additionally, I think the points are too small, so I will change them manually through arguments fed through the layers. Finally, I also add a `geom_line` which when combined in this instance, draws a line between the points, separated by the color specification, by default, `geom_` functions inherit their aesthetic from the layer previous, so the same `aes` specification is inherited from the higher level. 
 
```{r}
grouped_f %>% 
  mutate(wave = case_when(             ### Mutating to better cases, so they are parallel
    year == 1995 | year == 1996 ~ 1,
    year == 2000 | year == 2001 ~ 2,
    year == 2005 ~ 3,
    year == 2010 ~ 4
  )) %>% 
  ggplot(aes(x=wave, y = whostility, color = country)) +  ### Setting aesthetic
  geom_point(size = 2) +  # Scatterplot, with 2 as size of the dots
  geom_line() +          # make a line, inherits past aesthetics
  theme_bw() +        # theme_bw is the best
  scale_color_manual(values = c("cornflowerblue", "goldenrod")) +  ## Values are assigned alphabetically when set manually, so Japan gets first colour
  labs(title = "Hostility towards women as legislators", x = "WVS Wave", y = "Hostility") ## Creating labels
```
The values and precise aesthetics of a variable can be set with the function family `scale_AESTHETIC_TYPE`, which provides a large range of ways to modify the aesthetics. In the plot above I manually (therefore manual at the end) assign two values to the color aesthetic (therefore color in the middle). This function family can also modify labels and other elements. 

## Maps

You can also utilize the ggplot package, in conjunction with the sf package, to visualise maps. This requires shapefiles, which describes the general outlines of some countries. These can also be accessed through several packages, or downloaded from gadm manually. There are a lot of intricacies involving maps and shapefiles in general, but to use it with ggplot it is most common to have it as a `sf` type file, you can see that the map assignment in the code chunk below is piped into a `st_as_sf` function, this transform the `tera` (I believe it is) class object, into an `sf` class object. In some instances we would have to assign a crs as well, but that's perhaps too much complicated to talk about right now. Note also that the `ggplot` layer is itself empty, because the data and aesthetic mapping needs to be from a shapefile function of `geom_sf`. 
```{r}
library(geodata) # for gadm download
library(sf)

nor_map <- gadm(country = "NO", level = 1, path = "./") %>% 
  st_as_sf()

ggplot() +  ## in map data the ggplot is empty
  geom_sf(data = nor_map) + 
  theme_void() # common theme to eliminate all lines

```
Aesthetics are possible, filling the map to make a cloropleth for instance. You can also left join data that are not of a GIS character straight into this dataframe, then use those variables as aesthetics, but for now lets just make a new variable based on whether I have visited the county.

```{r}
nor_map <- nor_map %>% 
  mutate(me_there = case_when(
    HASC_1 == "NO.AA" ~ "Yes",
    HASC_1 == "NO.AK" ~ "Home",
    HASC_1 == "NO.BU" ~ "Yes",
    HASC_1 == "NO.FI" ~ "No",
    HASC_1 == "NO.HE" ~ "Yes",
    HASC_1 == "NO.HO" ~ "Yes",
    HASC_1 == "NO.MR" ~ "No",
    HASC_1 == "NO.NO" ~ "No",
    HASC_1 == "NO.NT" ~ "No",
    HASC_1 == "NO.OF" ~ "Yes",
    HASC_1 == "NO.OP" ~ "Yes",
    HASC_1 == "NO.OS" ~ "Yes",
    HASC_1 == "NO.RO" ~ "Yes",
    HASC_1 == "NO.SF" ~ "Home",
    HASC_1 == "NO.ST" ~ "Yes",
    HASC_1 == "NO.TE" ~ "Yes",
    HASC_1 == "NO.TR" ~ "No",
    HASC_1 == "NO.VA" ~ "Yes",
    HASC_1 == "NO.VF" ~ "Yes"
))

ggplot() + 
  geom_sf(data = nor_map, aes(fill = me_there)) + 
  theme_void() + 
  labs(title = "Mitt lille land", fill = "Have I been?") + 
  scale_fill_manual(values = c("goldenrod", "firebrick", "cornflowerblue"))
```
Note that the aesthetic argument is `fill` not `colour` (I have done that mistake several times). 

# Advanced methods

## For loops and flow control

A central part of programming in most languages is the flow control and for loops. For loops in R are slower than most other languages, and usually apply family of functions is preferred, since they can basically function in the same way as for loops, with the right specifications. But knowledge of loops can give you a tool to solve certain difficult tasks where you do not want to spend a couple of hours finding a relevant function. For loops consist of the function `for(iterator in sequence)`, the iterator is the thing that changes with each loop. A for loop is an instruction to do the same task over a set of times. 

Below is a simple for loop. First I create a vector of some numbers. The loop itself, I call the iterator i, and specify that I am looping over a sequence of numbers, from 1 to the length of the vector. I then use that operator as an index to the vector, so that for each iteration of the loop it prints the i vector position squared. 

```{r}

loopy_vector <- c(15,23,34,66,32)

for(i in 1:length(loopy_vector)){
  
  print(loopy_vector[i]^2) ### Note that unlike outside the loop, it does not print results automatically

}


```
For loops can interact with global variables, and assignment inside the loops are tracked in the global environment, and change variables you have in your environment, unlike lapply, which usually only changes return values. 

### flow control

For loops are just one version of repeating code, `while` and `repeat` also do similar things, however, I have never used them, and highly doubt the reader will do as well. 

Flow control refers to ways in which you control a for loop or other similar iterative processes. These keywords are highlighted differently than other keyterms (in my instance they are red (I think, I cant see colours), but it varies with RStudio theme). These words are amonst others:

- `if(){}` code in the bracket only executed if the logical test within parenthesis returns TRUE, can be followed by else
- `else{}` follows the end of the if code bracket, code is executed if the test in the if function is returned as FALSE
- `next`, usually nested within an if or else test, skips to the next iteration of the loop. 
- `break`, exits the loop completely, stopping the iteration over loops, *needs* to be present in a repeat loop, or it will never stop
- `return`, specifies what object or element within the function or loop that should be returned when the iteration is complete, by default the function will return the last object created within it.  


## Functional programming with anonymous functions

If you are uncertain about how to do certain data wrangling processes, it is always possible to write your own function, or own generic procedure for a single problem. Using the `lapply` function, or the general family of `apply` functions, one can assign a very efficient function to be applied over a set of elements in a list. The letter in front usually specifies what type of data you will get back, where `lapply` is the easiest to use, since it just returns a list, is therefore less likely to throw a merger error. `sapply` will attempt to return the simplest datatype possible, `vapply` will return a vector. 


```{r}

lapply(loopy_vector, FUN = function(x){ ## x serves here as our "iterator"
  
  y <- x^2
  
  return(y)
  
})

```
Note that the output from the function is a list, with nested singular values, a normal method is to wrap a `lapply` function in an `unlist` function, or to feed the `lapply` to one using a pipe. 

If you know what you unit you want to do the same thing over and over again for, you do practically anything with an `apply` function. A good idea is to practice situations with it, because then when you are faced with an unfamiliar problem, you can go straight to work fixing the issue, rather than searching for obscure packages with functions that do the thing you want. 

## String manipulation

I will not go into details in this section, but will make it clear that character variables can be utilized in many different ways through what is known as regular expressions or regex, which are lines of very silly text which can be used as pattern matching to strings as data. 

```{r}
this_is_a_string <- "hi my name is Gard and I have clinical depression, please help"

## Regex looks insane, so when i for instance want to check whether there is a capitalized name in this string

library(stringr)

## Now i check the string on a pattern that matches to any substring that 
## starts with a single capitalized letter followed by 2 or more uncapitalized
## letters.

str_detect(this_is_a_string, "[A-Z]{1}[a-z]{2,}")

## Other functions could do similar stuff, like extract the match

str_extract(this_is_a_string, "[A-Z]{1}[a-z]{2,}")


## Same but for a comma and then anything

str_extract(this_is_a_string, "\\,.*")

## Anything following a comma, but not the comma

str_extract(this_is_a_string, "(?<=\\,).*") 

# Note the space is still there

```
Regex can get extremely complex, and string manipulation in general is a very advanced topic in terms of common R usage, I still have to google many of the characters used in these patterns, and to teach away how regex works properly could probably be an entire course or two in and of itself. 

## Keyboard shortcuts and tips

- ctrl + shift + m gives a pipe %>% 
- alt + - gives an assignment operator <- 
- shift + any arrow key, highlights text as you move the text entry thing
- ctrl + shift + left/right highlights while skipping elements, so you can highlight entire words
- If you have an entire section highlighted, you can wrap it in ", (, [, etc. By just pressing that button while having the thing highlighted
- ctrl + alt + up/down, gives you more cursors, so you can add more than one line at a time. Useful for repeated entries of the same thing

There is an extensive style guide, but generally R custom is to use what is called snake case for variable and object naming. The name "SurvData" used earlier violates this by being pascalcase, a strict snake case version would be "surv_data". point being, avoid upper case letters, bind words with an underscore. Same custom goes for filenaming of R-scripts and datasets. 

## Structuring your scripts

Scripts are there for reproduction, you could technically run your entire code through the console, but scripts exist to ensure that you can reproduce your results, and that others can reproduce your results. For that reason it is a good idea to have a good structure to your scripts. The main elements that you should always keep in mind is: Linearity, outline, readability, and comments. Linearity in this instance refers to a technical requirement as much as a readers requirement. It is common when beginning to write longer scripts, that you start to run elements of the code out of order. That is fine in and of itself, but becomes a problem when you start writing new code in places it does not make sense to, and when code that depends on variables specified later in your script is added. R is almost always run sequentially, if I am reviewing someone else's script, I will just highlight the entire script, and run all of it. You should ensure that your scripts will run on the first time they are done this way, therefore, be mindful of where your variable specifications are, and of where you change variables, where you create them, and when your output needs them to be used. 

Outline in this instance refers to the idea that you separate your script into easily identifiable chunks. This can actually be done in the comments. If you comment 4 or more #, followed by text and then 4 or more # or -, it will create a section in your script, which can be visible from the outline command.

![outline](./images/outliners.png)

You can then use this table of contents to more easily navigate the structure of your script. Additionally, creating such outlines is good to ensure the previously mentioned requirement of linearity.

When it comes to readability, it is important that you take a couple of key steps to ensure that your code is light on the eyes and the code viewer. Remember to have proper spacing between elements of your code, below are a couple of examples of good and bad practice when it comes to writing code with spacing.

```{r}
df_example2<-df_example%>%filter(happy==FALSE) ## BAD
df_example_2 <- df_example %>% filter(happy == FALSE) ## Good

## Note the spacing between opeerators, but not between parenthesis and text. 

df_example2 <- df_example %>% filter(happy == FALSE,age == 27) ## BAD
df_example2 <- df_example %>% filter(happy == FALSE, age == 27) ## GOOD

## Note that spacing after the commas, just like normal writing, is good practice

```

Additionally, your code should not be too wide. Perhaps you have noticed a strange grey line running down your script editor. This is the recommended width by posit, the company who makes RStudio. while it is generally fine to break this line once or twice, it should serve as a guide to how wide your script should be, to ensure that it is easy to follow. Linebreaks are automatically spaced in a nice way when you use them inside functions between arguments, as is evident from some of the code demonstrated here. Below is also an example of really bad and embarassing code from my MA thesis. 

![really bad stuff](./images/bad_stuff.png)


Finally, it is good practice to comment your code regularly. A # will turn everything on the line that follows it into a comment. Make sure that you provide the reader with some context as to how you are proceeding. 

## Quarto - creating nice documents in RStudio with code

The document you are currently reading is made through the quarto framework, which is developed in RStudio. This framework allows for seamless creation of pdf, html, and other types of files, from your RStudio IDE. The outline files for these documents end in .qmd, and you can create a new one from the top corner of your IDE. 

![here](./images/heres_quarto.png)

When clicking new quarto document, you will arrive at a prompt to create one of several types of documents, the current one is an html document, but through quarto you can also create latex specified documents (I have a friend who wrote their Master thesis in RStudio, you can also use bibref through it). Quarto can also make presentations through beamer, revealjs and other engines for making presentations (even powerpoint can be made through RStudio). Following the creation of the file, you specify several settings in the YAML header, which is at the top, here you can specify settings that affect the entire document, and the way it is rendered. The YAML header for this document for instance looks like this: 

![yaml](./images/yaml_header.png)