---
title: "R Tutorial"
author: "Gard Olav Dietrichson"
format: html
execute: 
  warning: false
---

# Introduction

Welcome to a (more or less) thorough walkthrough of R in applied areas. Covers the sections: General Structure, Data import, Simple Data checking, Data Wrangling, Plotting, and advanced R methods. 

# General structure

## Data Types

Information takes several forms in R, all attached with a class, the basic forms of data that needs to be known are numeric (will appear as "double" sometimes, this is related to memory allocation, but just means more decimals numeric), integer (whole numbers), factor (text but with numbers assigned to certain values), character, and boolean (logical, true or false). Additionally there are a class of missing values, but they are all covered by NA.

```{r}
1 + 1 ### Comments are in hashtags 

"hello" ## Strings or characters

NA

## NA can also have type specific values, but these are *never* used

NA_character_
NA_complex_ ## All yield same output

## Boolean datas

TRUE
FALSE

```
All entries in a script (which is just a text file), are sent to the console which interprets the text. There are also certain special datatypes, such as NULL for empty (which is not the same as missing), Inf for infinite values (dividing by zero for instance), and NaN (Not a Number, errors in certain computational processes)

## Assignment

While strictly speaking a functional programming language, object oriented programming is highly supported, and the norm for usage amongst data scientists. Most outputs in the language will be assigned a name, and appear in the global environment (in one of the other windows in your RStudio IDE). This is done either with a `=` or (more commonly, due to specific interactions with an equals sign) with the `<-` known as the assignment operator.  

```{r}
hello <- 23

hello ## Calling the object prints the content by default
```


### Other operators

Other operators become important when discussing logical tests or other mathematical operations. 

- +, -, /, *, ^ all does what you would expect
- == Strict logical test, is this equal to the other thing, response in boolean
- != Strict anti-logical test, is this thing different to the other thing, response in boolean
- <, >, <=, >=, less than, larger than, less or equal than, larger or equal than, logical tests, returns boolean
- &, |, Binding operators for more logical tests & (AND operator) requires both elements to be true, while | (OR operator) requires only one to be true
- %in% Contained logical test, is there a unit in this other element which is strictly equal to the first thing.

```{r}
"Yes" == "Yes"

"Yes" == "No"

"Yes" != "Yes"

"Yes" != "No"

"Yes" %in% c("No", "no", "Maybe", "Yes")

```


## Vectors

All complex data of interest to us in R is either a vector, a list or a dataframe (Matrix and Arrays exist, but are not relevant to this scope). Vectors are uni-dimensional with strict same type requirement, dataframes are two-dimensional, and lists are uni-dimensionals with no requirement for consistency among entries.

```{r}
# Vector
a_vector <- c(1, 3, 5, 6, 7,1)

```
All dimensional datatypes can be indexed using brackets, to call individual elements of the object

```{r}
a_vector[4] ## Fourth number of the vector above
```
## Dataframes

Two dimensional data structures are referred to as dataframes, and consists of rows of units across columns of variables. Columns are essentially vectors, and must therefore be of the same type. Indexing is done using brackets, and has to be separated with a comma, first number is the rownumber, second is the column number. Empty spaces extracts the whole row/column.

```{r}
df_example <- data.frame(names = c("Gard", "Grad", "Gard Olav", "the Notorious G.A.R.D."),
                         age = c(27, 23, 21, 19),
                         fav_movie = c("The Adolescence of Utena", "12 Angry Men", "Inception", "Straight outta Compton"),
                         happy = c(FALSE, TRUE, FALSE, TRUE))

df_example[1,]

df_example[c(1,2),]

df_example[,1]

df_example[,1:2]

df_example[3,2]

## Selecting on names

df_example[,c("fav_movie")]

## Index tool

df_example$age

df_example$happy

```

The most common process of accessing variables when using dataframe objects however, is to use the $ sign, which is an indexation tool for column names.

## Lists

Lists can have anything in them, even dataframes or vectors, with no rules as to consistency over elements. Elements are then stored unidimensionally, accessed with double brackets for a relevant element. 

```{r}
ex_list <- list(df_example, a_vector)

ex_list[[2]]

```
Lists are not often used in our branch of datascience, but will appear as accidental outputs to certain functions and processes, so its good to know what they are. 

## Functions

Functions are, the single most important aspect that exists in R, they are not commands, this ain't stata. Understanding the universe of functions afforded to you through a multitude of packages is the key to conducting efficient data processing in R. 

A function has the general form of `function_name(argument_1, argument_2, ... argument_n)`. All functions have clearly defined arguments, which control the outcome of the function. What these arguments are, are specified in the help file, which can be called upon by typing `?function` in the console. Sometimes these arguments are data oriented, the first argument in many tidyverse functions for instance, is a dataframe. Others are control oriented, meaning that they control specifications of the function itself. The function `mean()` for instance, takes three primary arguments, one is a vector of numbers (it also strictly only accepts dataypes that can be interpreted as numbers, so no character means), the second is a trim value, which is set to trim certain percentages of each side of the numerical distribution. Finally there is a na.rm argument, which controls whether or not to remove missing values before computing, if not done, a vector with missing values will return an error from the function. 

```{r}
mean(a_vector)

mean(a_vector, trim = .2)

new_vector <- c(1,3,5,6,7,3,NA)

mean(new_vector)

mean(new_vector, na.rm = TRUE)
```


# Simple Analysis

Before you can use R to conduct data wrangling or very directed plotting, you should utilize some functions to explore the data you are interested in. In this section I will utilize a singular dataset to demonstrate some of the simpler data exploring techniques. 

First thing to address is data import. Based on the type of data applied, you utilize a particular function, usually found in the `haven` package, or just in base R. The most common datatypes are .csv for comma separated files, .xlsx files for excel (lots of strange things can happen when reading these files, be mindful), .sav for stata files, additionally, R has a set of unique filetypes that are relevant, .rda, .rds, and .rdata, .rda and .rdata are loaded with a simple `load` function, while .rds is read using a `readRDS` function.  Each has an appropriate function for reading the relevant data. I load in a compiled set from WVS from South Korea and Japan here.

```{r}

## Note the file path specification is also very specific. Two dots signal
## a directory above your current. So this data is found in 
## the parallell folder MA thesis, which is parallell to the folder two layers
## above the current directory, then with a specified path to find it.

library(haven)

load("../../MA thesis/Data and R scripts/Survey Data/SurvData.Rdata")



```


## Simple data structure functions

For simple sakes, we can explore the structure of the dataset through just clicking on it in the global environment, the same effect can be achieved by using the `view` function, either in the console or in the script (but it can get annoying when re-running it inside the script). For a more systematized view of the data, the `head`, `str`, and `summary` functions are often used. Try them on your own data, but I will utilize head here, because it looks the best in this type of document.

```{r}
head(SurvData)
```

This dataset is clearly already been processed a bit, since a WVS would probably have far more variables. But for now we can tell that there are a set of variables that could interest us. I often recheck the precise names by calling the `names` function, which just prints out the names of the columns in the dataset. When specifying columns in tidyverse functions this can be a helpful reminder.

```{r}
names(SurvData)
```
This is all on an overview level, to give you an idea of what the data looks like and how to further explore it. When it comes to familiarizing yourself with aspects of the dataset, the $ sign for indexing columns becomes helpful. I usually pair this with a `table` function, or a `summary` function, which gives me an idea of what the distribution looks like. For simple interaction exploration, `table` also accepts several variables so you can see where they overlap

```{r}
summary(SurvData$gender)

table(SurvData$gender)

table(SurvData$gender, SurvData$whostility)
```
Also, you can use some very simple plots to help you view the distribution, such as a density plot, or a histogram for certain values.

```{r}
plot(density(SurvData$whostility))

hist(SurvData$whostility)
```
Additionally, some dataforms have labels attached to their columns, this is normal for data imported from spss, or stata. These labels provide information about the coding, without searching the codebook, these can be found with the `attributes` function. I load a dataset here, which has this type of labels. This data will also be used later.

```{r}
load("../../advanced_statistics_phd/my_data/simple_set.rda")

attributes(joined_sets2$straff)

```

 It is also in Norwegian, apologies for that. 

# Data wrangling

Transforming data is best done through the tidyverse system. The most key aspect of this framework is the usage of the pipe operator `%>%`, which takes the output from whatever is in front of the pipe, and gives it as the first argument in the function following the pipe (As a sidenote, if you want to call the preceeding object again, you can use a . to put it more places). Tidyverse allows for many useful simplifications, such as selecting columns by names without complex indexing or name wrapping. Selecting in tidyverse also supports something called tidyselect functions, which allows you to select certain variables based on a pattern, such as the `starts_with` function, which selects all variables that start with a specified string pattern. If you have a vector of your variable names you can also use the `all_of` function to select all of thoes. In this section I will use the dataset previously outlined in the section on simple analysis. I will go through the most common data wrangling functions that I use in my processes.

```{r}
library(tidyverse)

## A simple pipe explanation

SurvData %>% 
  pull(whostility) %>% 
  mean(na.rm = TRUE)

# Is the same as 

mean(pull(SurvData, whostility), na.rm = TRUE)

```


## Mutating

Mutate is the dplyr function for changing variables in a dataset. This means that the function can both create and modify existing variables in the dataset. As we saw in the last structure of our dataset, the gender variable is operationalized as 1 for men and 2 for women, we usually prefer a binary for this, so let us change that variable in a new dataset. Note that variable selecting in tidyverse functions are often quite straightforward, most if not all functions can specify variables by just writing them out in plain text as seen below. 

```{r}
SurvData <- SurvData %>% 
  mutate(gender_subtracted = gender - 1)

## Lets compare the outputs to ensure successful transformation

table(SurvData$gender, SurvData$gender_subtracted)

table(SurvData$country)


```



### Ifelse and Case when mutations

One of the most common techniques for assigning new values, other than just committing straight mathematical operations on them, is to utilize a logical test to assign new values. The most simple way of doing this is the `ifelse` function. It, simply put, tests each element of a vector and then assigns a new value based on TRUE or FALSE response, the syntax is as follows `ifelse(logicaltest, value_if_TRUE, value_if_FALSE)`. If for instance we wanted the new gender variable to be a character based on gender we could do the following.

```{r}
SurvData <- SurvData %>% 
  mutate(gender_ifelsed = ifelse(gender == 2, "Woman", "Man"))

table(SurvData$gender, SurvData$gender_ifelsed)

```
This is doable for instances where the new variable is dichotomous, and while it is possible to put a second `ifelse` function inside the response for the results of the logical test, this can get extremely cluttered as several cases appear. Which is why the dplyr package, helpfully has the `case_when` function, which is a vectorized version of ifelse, the helpfile will even say so. `case_when` has a bit more complex of a syntax, where the logical test is followed by a ~ and then the assigned value, with commas separating all cases. 

```{r}
SurvData <- SurvData %>% 
  mutate(gender_cased = case_when(  ## Style guide recomends this style
    gender == 2 ~ "Woman",
    gender == 1 ~ "Man",
    gender == 3 ~ "Non-Binary"
  ))

table(SurvData$gender, SurvData$gender_cased)

```


### Across

If you have a long set of variables you want to do the same thing to, the `across` selection function can be very helpful. When used within a mutate function it can apply the same function unto several columns. The function assignment is signalled here with a ~ which is called a "purr-style" lambda function, but other ways of assigning the function is technically possible. The .x following signifies the place the column to change has in the function used in it.  

Across can also be extended with anonymous functions (that is technically what is being done behind the scenes), which I will cover later in the document. 

```{r}

vars_i <- names(joined_sets2)[16:29]

joined_sets2_mutated <- joined_sets2 %>% 
  mutate(across(all_of(vars_i), ~ haven::zap_labels(.x))) %>% ## Heres a fun interaction with haven...
  mutate(across(all_of(vars_i), ~ ifelse(.x > 2.5, "High", "Low")))


head(joined_sets2[,16:29])
#Compared with
head(joined_sets2_mutated[,16:29])


```



## Filtering

The second common thing to do when treating data is to filter it, keeping only certain observations. Again you would use a logical test in order to filter these things, so get used to writing these sorts of tests. I showed earlier that the data had two countries, lets make a new dataframe with only Japan. Note also that this function will remove lines, and as such, you should not overwrite the old dataframe when filtering, like I did in the mutate section.

```{r}
jap_surv <- SurvData %>% 
  filter(country == "Japan")

```

We can add more tests to filter it further, either with a binding logical operator, or a comma, but comma adds strict additional requirement. 

```{r}
jap_surv_w <- SurvData %>% 
  filter(country == "Japan", gender_ifelsed == "Woman")

table(jap_surv_w$country, jap_surv_w$gender_ifelsed)

women_or_japanese <- SurvData %>% 
  filter(country == "Japan" | gender_ifelsed == "Woman")

table(women_or_japanese$country, women_or_japanese$gender_ifelsed)

```


## Joining 

The most common iteration of joining is the `left_join` function. This joins a secondary dataset to the left of the first dataset. In order to join the datasets you must specify a set of "keys", a set of variables which the values in the left dataset is supposed to match to the right dataset. 


## Grouping and Aggregating

A common need is also to use lower level data to create data on summaries of aggregate level, like the average level of women respondents across the survey years. This also has a slightly complex syntax, but you first group the dataframe with the function `group_by`, which specifies a set of variables that units should be grouped into, and then feed that into the `summarise` (`summarize` works too for americans) function, which specifies the new variables you want to create on the aggregate level. These new variables need to be specified by functions that summarise elements however, such as `mean`. In this code I will make an aggregate of Japan and South Korea over the time periods, with the number of respondents in each, and the average degree of anti-women legislator sentiment.

```{r}
grouped_f <- SurvData %>% 
  mutate(whostility = ifelse(whostility < 0, NA, whostility)) %>%  # You can add the unchanged vector as a responsein an ifelse, Values less than 0 are missing categories in WVS
  group_by(country, year) %>% 
  summarise(respondents = n(),
            whostility = mean(whostility, na.rm = TRUE))


head(grouped_f)


```
Given that grouping is part of tidyverse, we can also use tidyselect within it. Lets say I want the mean of several variables.

```{r}

my_vars <- c("PolDisc", "interest", "whostility")

grouped_more <- SurvData %>% 
  mutate(across(all_of(my_vars), ~ ifelse(.x < 0, NA, .x))) %>% 
  group_by(country, year) %>% 
  summarise(across(all_of(my_vars), ~ mean(.x, na.rm = TRUE)))


head(grouped_more)

```
Here we see a NaN example, where a mean of nothing is not a number, since we specified na.rm to be true. 


## Pivoting

Say we have a dataset where differences in years are stored in columns, and we want a single row for each yearly observation. Or we have data that is more extensive than we want, and we want to limit the number of rows. This can be achieved through pivot functions, primarily `pivot_wider` and `pivot_longer` which have fairly intuitive names, they make the dataframe wider or longer, by making it shorter or narrower, respectively. The way to use them is to feed a dataframe into it, with a pipe of course, and then, depending on wider or narrower, you have to specify which columns the names go to or from, and where the values go to and from. For a wider pivot, the new names are all a class of variables, such as a character string for instance, when specifying `names_from`, this is the variable that the new columns will be named after. 

Lets use a different dataset for a more applicable situation. I am here loading a dataset from a survey which is meant to measure agreement within party structures, I transform the question values to instead be distance to the mean of the party. 


```{r}

## Will limit this dataset to only a single party, and a couple of questions and key variables
## Ignore the complex code, it might make sense later

vars_i <- names(joined_sets2)[16:29]

mean_values_for_transf <- joined_sets2 %>% 
  select(parti, tid, all_of(vars_i)) %>% 
  group_by(parti, tid) %>% 
  summarise(across(all_of(vars_i), ~ mean(.x, na.rm = TRUE))) %>% 
  pivot_longer(cols = all_of(vars_i), names_to = "question", values_to = "mean_vals")




```




# Plotting

To paraphrase the Beatles, all you need is ggplot. This framework allows you to plot essentially everything, although some packages obviously makes this plotting simpler, but most of them are just wrappers for ggplot code (even some python and Julia packages use ggplot in R as a backend). You could write entire books worth of information on plotting with ggplot. In fact someone did, I have it if you're interested. 

![neat book](./images/healy_vis.jpeg)

## Layers

It is prudent to think of visualisations in ggplot as layered. These layers are bound together with plus signs, and each construct a layer ontop of the plot or other visualisation you want to make. The most basic general layer that all ggplots need is the base layer of the `ggplot` function itself. In this function you specify your dataset, and the general aesthetics. Within the `ggplot` function (and most `geom_` derivatives) there is an `aes` argument, this is an aesthetic argument, it specifies what variable from the dataset should represent what element of the plot. Based on the plot type the type of aesthetic accepted varies, but a list of potential aesthetics are: x, y, color, fill, shape, size, linewidth, linetype, etc. Following this, you add a layer that specifies what type of plot you want, this is done through a family of `geom_` functions such as `geom_point` for scatterplot, `geom_bar` for a barplot, `geom_boxplot` for a boxplot with distributions, etc. Which you use depends on how you want to visualise. If you are curious about the options, type `geom_` in R, and switch through the optins that it auto-completes to. Lets use this information to build a scatterplot of the first grouped dataframe created earlier.

```{r}
grouped_f %>% ### Obviously you can pipe it in 
  ggplot(aes(x=year, y=whostility, color = country)) + 
  geom_point()

```
 Very ugly, but it does the job. We can now add a couple of layers to improve the look. There are several themes that can be used by default, such as `theme_bw` which is my favourite, just add the function in a layer. Additionally we can modify the labels of the plot through the `lab` function as a separate layer. 
 
```{r}
grouped_f %>% 
  ggplot(aes(x=year, y=whostility, color = country)) + 
  geom_point() + 
  theme_bw() + 
  labs(title = "Here's a Title", x = "Time period", y = "Anti-women sentiment", color = "Region")
```
 For the final plot, I will also do some changes to the datastructure so that it visualises a bit better, there's perhaps too much space between the timings as it is. Additionally, I think the points are too small, so I will change them manually through arguments fed through the layers. Finally, I also add a `geom_line` which when combined in this instance, draws a line between the points, separated by the color specification, by default, `geom_` functions inherit their aesthetic from the layer previous, so the same `aes` specification is inherited from the higher level. 
 
```{r}
grouped_f %>% 
  mutate(wave = case_when(
    year == 1995 | year == 1996 ~ 1,
    year == 2000 | year == 2001 ~ 2,
    year == 2005 ~ 3,
    year == 2010 ~ 4
  )) %>% 
  ggplot(aes(x=wave, y = whostility, color = country)) + 
  geom_point(size = 2) + 
  geom_line() + 
  theme_bw() + 
  scale_color_manual(values = c("cornflowerblue", "goldenrod")) + 
  labs(title = "Hostility towards women as legislators", x = "WVS Wave", y = "Hostility")
```
The values and precise aesthetics of a variable can be set with the function family `scale_AESTHETIC_TYPE`, which provides a large range of ways to modify the aesthetics. In the plot above I manually (therefore manual at the end) assign two values to the color aesthetic (therefore color in the middle). This function family can also modify labels and other elements. 

## Maps

You can also utilize the ggplot package, in conjunction with the sf package, to visualise maps. This requires shapefiles, which describes the general outlines of some countries. These can also be accessed through several packages, or downloaded from gadm manually. 

```{r}
library(geodata) # for gadm download
library(sf)

nor_map <- gadm(country = "NO", level = 1, path = "./") %>% 
  st_as_sf()

ggplot() +  ## in map data the ggplot is empty
  geom_sf(data = nor_map) + 
  theme_void() # common theme to eliminate all lines

```
Aesthetics are possible, filling the map to make a cloropleth for instance. You can also left join data that are not of a GIS character straight into this dataframe, then use those variables as aesthetics, but for now lets just make a new variable based on whether I have visited the county.

```{r}
nor_map <- nor_map %>% 
  mutate(me_there = case_when(
    HASC_1 == "NO.AA" ~ "Yes",
    HASC_1 == "NO.AK" ~ "Home",
    HASC_1 == "NO.BU" ~ "Yes",
    HASC_1 == "NO.FI" ~ "No",
    HASC_1 == "NO.HE" ~ "Yes",
    HASC_1 == "NO.HO" ~ "Yes",
    HASC_1 == "NO.MR" ~ "No",
    HASC_1 == "NO.NO" ~ "No",
    HASC_1 == "NO.NT" ~ "No",
    HASC_1 == "NO.OF" ~ "Yes",
    HASC_1 == "NO.OP" ~ "Yes",
    HASC_1 == "NO.OS" ~ "Yes",
    HASC_1 == "NO.RO" ~ "Yes",
    HASC_1 == "NO.SF" ~ "Home",
    HASC_1 == "NO.ST" ~ "Yes",
    HASC_1 == "NO.TE" ~ "Yes",
    HASC_1 == "NO.TR" ~ "No",
    HASC_1 == "NO.VA" ~ "Yes",
    HASC_1 == "NO.VF" ~ "Yes"
))

ggplot() + 
  geom_sf(data = nor_map, aes(fill = me_there)) + 
  theme_void() + 
  labs(title = "Mitt lille land", fill = "Have I been?") + 
  scale_fill_manual(values = c("goldenrod", "firebrick", "cornflowerblue"))
```


# Advanced methods

## For loops and flow control

A central part of programming in most languages is the flow control and for loops. For loops in R are slower than most other languages, and usually apply family of functions is preferred, since they can basically function in the same way as for loops, with the right specifications. But knowledge of loops can give you a tool to solve certain difficult tasks where you do not want to spend a couple of hours finding a relevant function. For loops consist of the function `for(iterator in sequence)`, the iterator is the thing that changes with each loop. A for loop is an instruction to do the same task over a set of times. 

Below is a simple for loop. First I create a vector of some numbers. The loop itself, I call the iterator i, and specify that I am looping over a sequence of numbers, from 1 to the length of the vector. I then use that operator as an index to the vector, so that for each iteration of the loop it prints the i vector position squared. 

```{r}

loopy_vector <- c(15,23,34,66,32)

for(i in 1:length(loopy_vector)){
  
  print(loopy_vector[i]^2) ### Note that unlike outside the loop, it does not print results automatically

}


```
For loops can interact with global variables, and assignment inside the loops are tracked in the global environment, and change variables you have in your environment, unlike lapply, which usually only changes return values. 

## Functional programming with anonymous functions

If you are uncertain about how to do certain data wrangling processes, it is always possible to write your own function, or own generic procedure for a single problem. Using the `lapply` function, or the general family of `apply` functions, one can assign a very efficient function to be applied over a set of elements in a list. The letter in front usually specifies what type of data you will get back, where `lapply` is the easiest to use, since it just returns a list, is therefore less likely to throw a merger error. `sapply` will attempt to return the simplest datatype possible, `vapply` will return a vector. 


```{r}

lapply(loopy_vector, FUN = function(x){ ## x serves here as our "iterator"
  
  y <- x^2
  
  return(y)
  
})

```
Note that the output from the function is a list, with nested singular values, a normal method is to wrap a lapply function in an `unlist` function, or to feed the lapply to one using a pipe. 

## String manipulation

I will not go into details in this section, but will make it clear that character variables can be utilized in many different ways through what is known as regular expressions or regex, which are lines of very silly text which can be used as pattern matching to strings as data. 

```{r}
this_is_a_string <- "hi my name is Gard and I have clinical depression, please help"

## Regex looks insane, so when i for instance want to check whether there is a capitalized name in this string

library(stringr)

## Now i check the string on a pattern that matches to any substring that 
## starts with a single capitalized letter followed by 2 or more uncapitalized
## letters.

str_detect(this_is_a_string, "[A-Z]{1}[a-z]{2,}")

## Other functions could do similar stuff, like extract the match

str_extract(this_is_a_string, "[A-Z]{1}[a-z]{2,}")


## Same but for a comma and then anything

str_extract(this_is_a_string, "\\,.*")

## Anything following a comma, but not the comma

str_extract(this_is_a_string, "(?<=\\,).*") 

# Note the space is also there

```

## Keyboard shortcuts and tips

- ctrl + shift + m gives a pipe %>% 
- alt + - gives an assignment operator <- 
- shift + any arrow key, highlihgts text as you move the text entry thing
- ctrl + shift + left/right highlights while skipping elements, so you can highlight entire words
- If you have an entire section highlighted, you can wrap it in ", (, [, etc. By just pressing that button while having the thing highlighted

There is an extensive style guide, but generally R custom is to use what is called snake case for variable and object naming. The name "SurvData" used earlier violates this by being pascalcase, a strict snake case version would be "surv_data". point being, avoid upper case letters, bind words with an underscore. Same custom goes for filenaming of R-scripts and datasets. 
