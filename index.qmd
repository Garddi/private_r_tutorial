---
title: "R Tutorial"
author: "Gard Olav Dietrichson"
format: html
execute: 
  warning: false
toc: true
toc-location: right
number-sections: true
theme:
  light: flatly
  dark: darkly
---

# Introduction

Welcome to a (more or less) thorough walkthrough of R in applied areas. Covers the sections: General Structure, Data import, Simple Data checking, Data Wrangling, Plotting, and advanced R methods. 

# General structure

## Data Types

Information takes several forms in R, all attached with a class, the basic forms of data that needs to be known are numeric (will appear as "double" sometimes, this is related to memory allocation, but just means more decimals numeric), integer (whole numbers), factor (text but with numbers assigned to certain values), character, boolean (logical, true or false), and several types of Date data. Additionally there are a class of missing values, but they are all covered by NA.

```{r}
1 + 1 ### Comments are in hashtags 

"hello" ## Strings or characters

NA

## NA can also have type specific values, but these are *never* used

NA_character_
NA_complex_ ## All yield same output

## Boolean datas

TRUE
FALSE

```
All entries in a script (which is just a text file), are sent to the console which interprets the text. There are also certain special datatypes, such as NULL for empty (which is not the same as missing), Inf for infinite values (dividing by zero for instance), and NaN (Not a Number, errors in certain computational processes)

## Assignment

While strictly speaking a functional programming language, object oriented programming is highly supported, and the norm for usage amongst data scientists. Most outputs in the language will be assigned a name, and appear in the global environment (in one of the other windows in your RStudio IDE). This is done either with a `=` or (more commonly, due to specific interactions with an equals sign) with the `<-` known as the assignment operator.  

```{r}
hello <- 23

hello ## Calling the object prints the content by default
```
There also exists a `<<-` assignment operator, which can assign objects on a global scope within function environments, but this word salad is too advanced for this simple tutorial.

### Other operators

Other operators become important when discussing logical tests or other mathematical operations. 

- +, -, /, *, ^ all does what you would expect
- == Strict logical test, is this equal to the other thing, response in boolean
- != Strict anti-logical test, is this thing different to the other thing, response in boolean
- <, >, <=, >=, less than, larger than, less or equal than, larger or equal than, logical tests, returns boolean
- &, |, Binding operators for more logical tests & (AND operator) requires both elements to be true, while | (OR operator) requires only one to be true
- %in% Contained logical test, is there a unit in this other element which is strictly equal to the first thing.
- ! in general reverses the proceeding logical test, so if you write `!("YES" == "YES")` it will ask if the thing inside the parenthesis _is not_ the case, reversing the logical test. 
- :, integer operator, creates a vector of elements between the starting points. `1:7` will create a vector of whole numbers from 1 to and including 7
- `%%`, modulus operator, divides number preceeding it by number following it, and returns the remainder, so `12 %% 5` will return 2, since 5*2 = 10, with 2 as the remainder. 

```{r}
"Yes" == "Yes"

"Yes" == "No"

"Yes" != "Yes"

"Yes" != "No"

"Yes" %in% c("No", "no", "Maybe", "Yes")

### R is also what is called "loosely" typed, so this works

3 == "3" ## Equal even tho they are different types of data

```

Logical tests are also relevant through certain functions, these functions all return either TRUE or FALSE values, which is useful in several cases, inside other functions or for flow control in loops (all of this is covered later). These functions include `is.na`, `is.empty`, `is.character`, `is.numeric`, etc. Lots of functions to explore that start with `is.`

## Vectors

All complex data of interest to us in R is either a vector, a list or a dataframe (Matrix and Arrays exist, but are not relevant to this scope). Vectors are uni-dimensional with strict same type requirement, dataframes are two-dimensional, and lists are uni-dimensionals with no requirement for consistency among entries.

```{r}
# Vector
a_vector <- c(1, 3, 5, 6, 7,1)

```
All dimensional datatypes can be indexed using brackets, to call individual elements of the object

```{r}
a_vector[4] ## Fourth number of the vector above
```
## Dataframes

Two dimensional data structures are referred to as dataframes, and consists of rows of units across columns of variables. Columns are essentially vectors, and must therefore be of the same type. Indexing is done using brackets, and has to be separated with a comma, first number is the rownumber, second is the column number. Empty spaces extracts the whole row/column.

```{r}
df_example <- data.frame(names = c("Gard", "Grad", "Gard Olav", "the Notorious G.A.R.D."),
                         age = c(27, 23, 21, 19),
                         fav_movie = c("The Adolescence of Utena", "12 Angry Men", "Inception", "Straight outta Compton"),
                         happy = c(FALSE, TRUE, FALSE, TRUE))

df_example[1,]

df_example[c(1,2),]

df_example[,1]

df_example[,1:2]

df_example[3,2]

## Selecting on names

df_example[,c("fav_movie")]

## Index tool

df_example$age

df_example$happy

```

The most common process of accessing variables when using dataframe objects however, is to use the $ sign, which is an indexation tool for column names.

## Lists

Lists can have anything in them, even dataframes or vectors, with no rules as to consistency over elements. Elements are then stored unidimensionally, accessed with double brackets for a relevant element. 

```{r}
ex_list <- list(df_example, a_vector)

ex_list[[2]]

```
Lists are not often used in our branch of datascience, but will appear as accidental outputs to certain functions and processes, so its good to know what they are. 

## Functions

Functions are, the single most important aspect that exists in R, they are not commands, this ain't stata. Understanding the universe of functions afforded to you through a multitude of packages is the key to conducting efficient data processing in R. 

A function has the general form of `function_name(argument_1, argument_2, ... argument_n)`. All functions have clearly defined arguments, which control the outcome of the function. What these arguments are, are specified in the help file, which can be called upon by typing `?function` in the console. Sometimes these arguments are data oriented, the first argument in many tidyverse functions for instance, is a dataframe. Others are control oriented, meaning that they control specifications of the function itself. The function `mean()` for instance, takes three primary arguments, one is a vector of numbers (it also strictly only accepts dataypes that can be interpreted as numbers, so no character means), the second is a trim value, which is set to trim certain percentages of each side of the numerical distribution. Finally there is a na.rm argument, which controls whether or not to remove missing values before computing, if not done, a vector with missing values will return an error from the function. 

```{r}
mean(a_vector)

mean(a_vector, trim = .2)

new_vector <- c(1,3,5,6,7,3,NA)

mean(new_vector)

mean(new_vector, na.rm = TRUE)
```
Many functions are part of packages which needs to be installed to your computer using the `install.packages` function, and then when utilized needs to be loaded into the namespace you are currently operaring in, using the reserved function `library`. Additionally, all functions (should at least) have a help file, which you can access by posing a `?` in front of the function. The help file will list the structure of the function, the necessary arguments, and their default values. Do note however, that these help files can be extremely confusing to read for anyone who does not already understand them. 


## Folder structure and saving results

If you want to save your resulting items, whether they be dataframes or images or tables, you need to have a good idea about what your folder structure looks like. Your console is usually set to be in a certain working directory. You can find this working directory, by using the `getwd` function.
```{r}
getwd()
```
From this working directory, you can access all files that are on your computer, if you know where they are relative to each other. Remember that all directories (or folders) are subfolders of other elements. To reach other files you navigate using / separators. So if I want to reach the files that are in a folder within my current working directory, I specify them with a / and then the subfolder. If I for instance want to know what files are in the images subfolder of my current working directory, I specify that as following.

```{r}
list.files("./images/") ## ./ means the current working directory
```
However, you are not limited to only subfolders in your current directory, by using two dots, instead of one, you go to the parent folder of your directory.

```{r}
list.files("../.")
```
Note that the folder which we saw as my working directory appears here as a directory. We can also look at the contents of parallel folders.

```{r}
list.files("../../advent_of_Code2022/scripts/")
```
In order to easily access most of your files and data, good folder structure is essential, I like to keep master scripts within the general topical folder, with subfolders for both data and results. When saving to these folders, you usually use a dedicated function that relates to the package eco-system that you are using, but the most simple datasets can be saved locally using `save` function, where you specify your dataset to save, and where you want to save it to. When saving outputs such as LateX tables, ggplot images and such, there are more dedicated functions.

## Added Notes on Indexing

Indexing variables and rows are keys to many of the later functions that will be discussed, and especially in terms of loops. Indexing a variable with the `$` operator is the same as extracting the vector that constitutes that variable, as such it itself can be indexed with the brackets. Note that since the resulting output is a vector, and therefore unidimensional, you only use one number. 

```{r}
df_example$names
df_example$names[3]

df_example$fav_movie
df_example$fav_movie[1]

```
Additionally, indexing is done primarily through numbers, and these numbers can and often should be, interacted with through object assignments. If I want the same indexed number several times for instance, I can store it as an object variable, maybe as a single letter (i, j, and k are common for this in loops, sometimes x as well),

```{r}
idx <- 3 # idx for indexer

df_example$names[idx]
df_example$happy[idx]

## For several numbers, still works fine

idx <- c(1,3)

df_example$names[idx]
df_example$fav_movie[idx]


```


# Simple Analysis

Before you can use R to conduct data wrangling or very directed plotting, you should utilize some functions to explore the data you are interested in. In this section I will utilize a singular dataset to demonstrate some of the simpler data exploring techniques. 

First thing to address is data import. Based on the type of data applied, you utilize a particular function, usually found in the `haven` package, or just in base R. The most common datatypes are .csv for comma separated files, .xlsx files for excel (lots of strange things can happen when reading these files, be mindful), .sav for stata files, additionally, R has a set of unique filetypes that are relevant, .rda, .rds, and .rdata, .rda and .rdata are loaded with a simple `load` function, while .rds is read using a `readRDS` function.  Each has an appropriate function for reading the relevant data. I load in a compiled set from WVS from South Korea and Japan here.

```{r}

## Note the file path specification is also very specific. Two dots signal
## a directory above your current. So this data is found in 
## the parallell folder MA thesis, which is parallell to the folder two layers
## above the current directory, then with a specified path to find it.

library(haven)

load("../../MA thesis/Data and R scripts/Survey Data/SurvData.Rdata")



```


## Simple data structure functions

For simple sakes, we can explore the structure of the dataset through just clicking on it in the global environment, the same effect can be achieved by using the `view` function, either in the console or in the script (but it can get annoying when re-running it inside the script). For a more systematized view of the data, the `head`, `str`, and `summary` functions are often used. Try them on your own data, but I will utilize head here, because it looks the best in this type of document.

```{r}
head(SurvData)
```

This dataset is clearly already been processed a bit, since a WVS would probably have far more variables. But for now we can tell that there are a set of variables that could interest us. I often recheck the precise names by calling the `names` function, which just prints out the names of the columns in the dataset. When specifying columns in tidyverse functions this can be a helpful reminder.

```{r}
names(SurvData)
```
This is all on an overview level, to give you an idea of what the data looks like and how to further explore it. When it comes to familiarizing yourself with aspects of the dataset, the $ sign for indexing columns becomes helpful. I usually pair this with a `table` function, or a `summary` function, which gives me an idea of what the distribution looks like. For simple interaction exploration, `table` also accepts several variables so you can see where they overlap

```{r}
summary(SurvData$gender)

table(SurvData$gender)

table(SurvData$gender, SurvData$whostility)
```
Another key function to use both when exploring your data, but also when you are troubleshooting issues later, is the `class` function. Quite simply it tells you what type of data the thing you are entering is. It works both on vectors/variables, which can let you know if you are dealing with a character variable or a numeric one, but also on complex objects in your environment, so you can know whether the object you are using is appropriate for certain functions. For instance, map visualisations usually requires objects of the `sf` class, not just dataframes. 

```{r}
class(SurvData$gender)

class(SurvData$country)

class(df_example$happy)

class(SurvData)


```

Furthermore, you can use some very simple plots to help you view the distribution, such as a density plot, or a histogram for certain values.

```{r}
plot(density(SurvData$whostility))

hist(SurvData$whostility)
```
Additionally, some dataforms have labels attached to their columns, this is normal for data imported from spss, or stata. These labels provide information about the coding, without searching the codebook, these can be found with the `attributes` function. I load a dataset here, which has this type of labels. This data will also be used later.

```{r}
load("../../advanced_statistics_phd/my_data/simple_set.rda")

attributes(joined_sets2$straff)

```

 It is also in Norwegian, apologies for that. 

# Data wrangling

Transforming data is best done through the tidyverse system. The most key aspect of this framework is the usage of the pipe operator `%>%`, which takes the output from whatever is in front of the pipe, and gives it as the first argument in the function following the pipe (As a sidenote, if you want to call the preceeding object again, you can use a . to put it more places). Tidyverse allows for many useful simplifications, such as selecting columns by names without complex indexing or name wrapping. In this section I will use the dataset previously outlined in the section on simple analysis. I will go through the most common data wrangling functions that I use in my processes.

```{r}
library(tidyverse)

## A simple pipe explanation

SurvData %>% 
  pull(whostility) %>% 
  mean(na.rm = TRUE)

# Is the same as 

mean(pull(SurvData, whostility), na.rm = TRUE)

```
## Dealing with bad names

Often times your data imported from excel will have really bad names, rife with capitalisation and spacing. Ideally you want a dataset where variables do not have these things, so it is easier to memorize and enter into R. If I face this issue I will often utilize the janitor package, which can clean names into something more coherent. If there is space for instance in the variable name, then it has to be wrapped in the ` signs, which can be complicated to type out when auto-complete is not available. 

```{r}
library(janitor)

df_example$`This Name is REALLY bad 120` <- "BAD"

names(df_example)

df_example <- clean_names(df_example)

names(df_example)

```



## Mutating

Mutate is the dplyr function for changing variables in a dataset. This means that the function can both create and modify existing variables in the dataset. As we saw in the last structure of our dataset, the gender variable is operationalized as 1 for men and 2 for women, we usually prefer a binary for this, so let us change that variable in a new dataset. Note that variable selecting in tidyverse functions are often quite straightforward, most if not all functions can specify variables by just writing them out in plain text as seen below. 

```{r}
SurvData <- SurvData %>% 
  mutate(gender_subtracted = gender - 1)

## Lets compare the outputs to ensure successful transformation

table(SurvData$gender, SurvData$gender_subtracted)

table(SurvData$country)


```
A common thing you might find with your dataset is that the variables have the wrong type. Perhaps a column that was meant to be a number was read as a character, because the idiot who made the excel sheet decided to spell out "no data" in the missing cells (these people should be executed). In many instances the simplest way of changing a variable, is to force it into another class by using an `as.` function, `as.numeric`, `as.character`, `as.Date`, and `as.double` are common variations of this. There is also the `factor` function, which allows you to create ordinal values. Where the `as.` functions are relatively simple (perhaps except the date one), factor also requires that you supply a vector of the rankings of the factor. 

```{r}

SurvData <- SurvData %>% 
  mutate(gender_text = as.character(gender))

class(SurvData$gender_text)

head(SurvData$gender_text)

```



### Ifelse and Case when mutations

One of the most common techniques for assigning new values, other than just committing straight mathematical operations on them, is to utilize a logical test to assign new values. The most simple way of doing this is the `ifelse` function. It, simply put, tests each element of a vector and then assigns a new value based on TRUE or FALSE response, the syntax is as follows `ifelse(logicaltest, value_if_TRUE, value_if_FALSE)`. If for instance we wanted the new gender variable to be a character based on gender we could do the following.

```{r}
SurvData <- SurvData %>% 
  mutate(gender_ifelsed = ifelse(gender == 2, "Woman", "Man"))

table(SurvData$gender, SurvData$gender_ifelsed)

```
This is doable for instances where the new variable is dichotomous, and while it is possible to put a second `ifelse` function inside the response for the results of the logical test, this can get extremely cluttered as several cases appear. Which is why the dplyr package, helpfully has the `case_when` function, which is a vectorized version of ifelse, the helpfile will even say so. `case_when` has a bit more complex of a syntax, where the logical test is followed by a ~ and then the assigned value, with commas separating all cases. 

```{r}
SurvData <- SurvData %>% 
  mutate(gender_cased = case_when(  ## Style guide recomends this style
    gender == 2 ~ "Woman",
    gender == 1 ~ "Man",
    gender == 3 ~ "Non-Binary"
  ))

table(SurvData$gender, SurvData$gender_cased)

```


### Across

If you have a long set of variables you want to do the same thing to, the `across` selection function can be very helpful. When used within a mutate function it can apply the same function unto several columns. The function assignment is signalled here with a ~ which is called a "purr-style" lambda function, but other ways of assigning the function is technically possible. The .x following signifies the place the column to change has in the function used in it.  

Across can also be extended with anonymous functions (that is technically what is being done behind the scenes), which I will cover later in the document. 

```{r}

vars_i <- names(joined_sets2)[16:29]

joined_sets2_mutated <- joined_sets2 %>% 
  mutate(across(all_of(vars_i), ~ haven::zap_labels(.x))) %>% ## Heres a fun interaction with haven...
  mutate(across(all_of(vars_i), ~ ifelse(.x > 2.5, "High", "Low")))


head(joined_sets2[,16:29])
#Compared with
head(joined_sets2_mutated[,16:29])


```

### New variables as vectors

Recall that a dataframe is practically just a vector of vectors. Each column, or variable is essentially just a vector with the same length as the total dataframe. This means you can also create new variables by creating a vector of the same length and in the same order as the dataframe. Most operations you do on vectors from the dataframe will result in this kind of mergible vector. If I add together the vector-variable of `interest` and the vector variable `poldisc`, the result is simply a vector of the same length and in the same order as the dataframe. Therefore, for simple variable changes, it is sometimes easier to just assign it like this, "manually" if you will.

```{r}
length(SurvData$interest + SurvData$PolDisc)

### Essentially, the following will remain true for 1:rows in dataframe

(SurvData$interest + SurvData$PolDisc)[8092] == SurvData$interest[8092] + SurvData$PolDisc[8092]


##

SurvData$pol_index <- (SurvData$PolDisc + SurvData$interest)/2 ## Making the mean out of the two

summary(SurvData$pol_index)

```
## Subsetting on variables

Later I will go through the process for selecting away cases by `filter`, but first, as previously mentioned, there is a family of `tidyselect` functions, of which the most relevant in our instance is the normal `select` function. There are two main ways of selecting within this function, you can either select the variables you *want*, or select away the variables you *don't want*. Within a select function, if you write the names of the variables you want, then those are the ones you get in the subset, but if you write a `-` sign in front, then it means the dataset without that (or those) variable(s).

```{r}
names(SurvData) ## Note which variables I do have

## First I only want country, gender and whostility

SurvData_sub <- SurvData %>% 
  select(country, gender, whostility)

names(SurvData_sub)  


## Now lets select away PolDisc and interest

SurvData_sub <- SurvData %>% 
  select(-PolDisc, -interest)


names(SurvData_sub)

```
Selecting in tidyverse is also supported by a range of other specific tidyselect functions, which allows you to select certain variables based on a pattern, such as the `starts_with` function, which selects all variables that start with a specified string pattern. If you have a vector of your variable names you can also use the `all_of` function to select all of those. This might not seem so useful here, but a general recommendation is that if you are repeating a process more than twice, it is always useful to write a simpler process like a function or a variable for it.  

```{r}
vars_i_want <- c("gender", "country", "whostility")

SurvData_sub <- SurvData %>% 
  select(all_of(vars_i_want))

names(SurvData_sub)

### On a pattern with starts_with

SurvData_sub <- SurvData %>% 
  select(starts_with("gender"))


names(SurvData_sub)

```



## Filtering

The second common thing to do when treating data is to filter it, keeping only certain observations. Again you would use a logical test in order to filter these things, so get used to writing these sorts of tests. I showed earlier that the data had two countries, lets make a new dataframe with only Japan. Note also that this function will remove lines, and as such, you should not overwrite the old dataframe when filtering, like I did in the mutate section.

```{r}
jap_surv <- SurvData %>% 
  filter(country == "Japan")

```

We can add more tests to filter it further, either with a binding logical operator, or a comma, but comma adds strict additional requirement. 

```{r}
jap_surv_w <- SurvData %>% 
  filter(country == "Japan", gender_ifelsed == "Woman")

table(jap_surv_w$country, jap_surv_w$gender_ifelsed)

women_or_japanese <- SurvData %>% 
  filter(country == "Japan" | gender_ifelsed == "Woman")

table(women_or_japanese$country, women_or_japanese$gender_ifelsed)

```



## Grouping and Aggregating

A common need is also to use lower level data to create data on summaries of aggregate level, like the average level of women respondents across the survey years. This also has a slightly complex syntax, but you first group the dataframe with the function `group_by`, which specifies a set of variables that units should be grouped into, and then feed that into the `summarise` (`summarize` works too for americans) function, which specifies the new variables you want to create on the aggregate level. These new variables need to be specified by functions that summarise elements however, such as `mean`. In this code I will make an aggregate of Japan and South Korea over the time periods, with the number of respondents in each, and the average degree of anti-women legislator sentiment.

```{r}
grouped_f <- SurvData %>% 
  mutate(whostility = ifelse(whostility < 0, NA, whostility)) %>%  # You can add the unchanged vector as a responsein an ifelse, Values less than 0 are missing categories in WVS
  group_by(country, year) %>% 
  summarise(respondents = n(),
            whostility = mean(whostility, na.rm = TRUE))


head(grouped_f)


```
Given that grouping is part of tidyverse, we can also use tidyselect within it. Lets say I want the mean of several variables.

```{r}

my_vars <- c("PolDisc", "interest", "whostility")

grouped_more <- SurvData %>% 
  mutate(across(all_of(my_vars), ~ ifelse(.x < 0, NA, .x))) %>% 
  group_by(country, year) %>% 
  summarise(across(all_of(my_vars), ~ mean(.x, na.rm = TRUE)))


head(grouped_more)

```
Here we see a NaN example, where a mean of nothing is not a number, since we specified na.rm to be true. 



## Joining 

The most common iteration of joining is the `left_join` function. This joins a secondary dataset to the right of the first dataset. In order to join the datasets you must specify a set of "keys", a set of variables which the values in the left dataset is supposed to match to the right dataset. These do not have to create an exact match between datasets, multiple points in the original set can match single values in the joining set, but the reverse makes things complicated. If a single row in the original set matches multiple rows of the new one, then duplicates are created. For this reason make sure that the original dataset is either on a sub-level hierarchically, or that there are unique and exact matches in the new dataset. 

The most common scenario to join values in my experience is when you have aggregate data that you want to join in to individual level data. Say I have survey result data from certain countries, and I want to join in some contextual variables. I could match that on just country and year variables. Below I utilize the survey in Norwegian parties, first I aggregate a frame, then join it. 

```{r}
joined_lim <- joined_sets2 %>% 
  mutate(eu = ifelse(eu %in% c(1,2,3,4,5), eu, NA)) %>% 
  select(kjonn, parti, tid, eu)

joined_lim_agg <- joined_sets2 %>% 
  mutate(eu = ifelse(eu %in% c(1,2,3,4,5), eu, NA)) %>% 
  group_by(parti, tid) %>% 
  summarise(mean_eu = mean(eu, na.rm = TRUE))

joined_lim[c(3,4,64,65,556,557,1231,1232,5688, 5689,6783, 6784),] ## Indexing some random rows, so we can see variance

joined_lim_joined <- joined_lim %>% 
  left_join(joined_lim_agg, by = c("parti", "tid")) # Here i join in the new set, i know that the parti and tid variables are in both sets

joined_lim_joined[c(3,4,64,65,556,557,1231,1232,5688, 5689,6783, 6784),] ## Indexing the same numbers
```
Since `mean_eu` was the only variable in the aggregated set except the key variables `parti` and `tid`, this is the only new column in the merged dataset. 

`left_join` is by far the most common way of joining a dataset when you want new information or elements to be added to your existing dataset. There also exist `right_join` and `full_join`, but `right_join` is practically the same as left, just less intuitive and more niche in my opinion, while `full_join` should not be used because I can rarely think of a scenario where it makes sense to use in relation to dataset specifications. 

The other relevant form of merging is when you have new data that is identical in structure to the old data, but they represent perhaps new observations to that data. In this instance you seek to bind the rows together, and suitably the function is just called `bind_rows`, if the datasets are structured properly, then you can just enter two datasets in that function and it should merge them. If they are not similar, you have to data wrangle them a bit. 

## Pivoting

Say we have a dataset where differences in years are stored in columns, and we want a single row for each yearly observation. Or we have data that is more extensive than we want, and we want to limit the number of rows. This can be achieved through pivot functions, primarily `pivot_wider` and `pivot_longer` which have fairly intuitive names, they make the dataframe wider or longer, by making it shorter or narrower, respectively. The way to use them is to feed a dataframe into it, with a pipe of course, and then, depending on wider or narrower, you have to specify which columns the names go to or from, and where the values go to and from. For a wider pivot, the new names are all a class of variables, such as a character string for instance, when specifying `names_from`, this is the variable that the new columns will be named after. 

Lets use a different dataset for a more applicable situation. I am here loading a dataset from a survey which is meant to measure agreement within party structures, I transform the question values to instead be distance to the mean of the party. 


```{r}

## Will limit this dataset to only a single party, and a couple of questions and key variables
## Ignore the complex code, it might make sense later

vars_i <- names(joined_sets2)[16:29] ## I know columns 16:29 are the ones I want

mean_values_for_transf <- joined_sets2 %>% 
  select(parti, tid, all_of(vars_i)) %>% 
  group_by(parti, tid) %>% 
  summarise(across(all_of(vars_i), ~ mean(.x, na.rm = TRUE))) %>% 
  pivot_longer(cols = all_of(vars_i), names_to = "question", values_to = "mean_vals")


head(mean_values_for_transf)


```

Note that we now only have four variables, but far more rows, instead of each question being a variable, each question is now a row for an individual. Lets do the same for the un-summarised version, then `left_join` this aggregate. 

```{r}
joined_sets2_i <-joined_sets2
joined_sets2_i$id <- 1:nrow(joined_sets2) ## If you want a fun story, ask why this is here........


new_joined <- joined_sets2_i %>% 
  mutate(across(all_of(vars_i), ~ haven::zap_labels(.x)),
         across(all_of(vars_i), ~ ifelse(!(.x %in% c(1,2,3,4,5)), NA, .x))) %>%  ## Likert scale, so outside 1-5 is missing vals
  pivot_longer(cols = all_of(vars_i), names_to = "question", values_to = "q_response") %>% 
  left_join(mean_values_for_transf, by = c("parti", "tid", "question")) ## Note the join so that the mean vals for the questions is there as well

dim(new_joined) ## Gets the dimensions of the object

```
As you can see the new frame has 136010 rows, and 23 columns, longer and narrower than the original one. Let's now turn that frame back to the original set, but now with "distance from party-year mean" as the variable on each question. Note that `pivot_longer` requires you to specify the columns that should be pivoted from, in addition to their destination, while `pivot_wider` knows that the names are going to be column names, and the values fill columns. That was poorly worded, remind me to come back to that and make it make sense.

As a sidenote, this section had to go through several revisions, because nothing works on first attempt, so do not be fooled into thinking this is an easy process, but practice makes perfect, and by using this language for yourself you eventually become quite apt at finding the source of your mistakes (and very good at googling). 

```{r}

dist_means_joined <- new_joined %>% 
  mutate(dist_score = q_response - mean_vals) %>% 
  select(-q_response, -mean_vals) %>% ### Removing these because they are reduntant and would create strange frames
  pivot_wider(names_from = "question", values_from = "dist_score")

head(joined_sets2[,vars_i])

head(dist_means_joined[,vars_i])

```

# Modelling

I will not spent any considerable time going into modelling here, given that outside of glm and lm, most operations will have a dedicated package with unique instructions that you will simply have to learn on your own. The main family of linear models, OLS estimations, are accessed through the `lm` function. This function accepts a formula as its first argument, which is specified with the dependent variable, followed by a tilde, then the specification of interest. We always store this as an object, a regression output, which we can interact with through generic functions such as summary, which gives a general output of the regression model.

```{r}

SurvData <- SurvData %>% 
  mutate(wave = case_when( ## Just using year will give a strange intercept
    year == 1995 | year == 1996 ~ 1,
    year == 2000 | year == 2001 ~ 2,
    year == 2005 ~ 3,
    year == 2010 ~ 4
  ))

model_1 <- lm(whostility ~ gender_text + country + wave + PolDisc + interest, data = SurvData)

summary(model_1)

```
From it we can see that women and south koreans in general are less hostile to women as legislators, we can underpin this with an interaction effect. which is specified with a multiplier sign. 

```{r}
model_2 <- lm(whostility ~ gender_text + country + wave + PolDisc + interest + country*gender_text, data = SurvData)

summary(model_2)
```
Effect of South Korea holds, but effect of being a woman becomes insignificant. For binary data we need a link function and a generalized linear model, which can be accesses through the function `glm` in R. Below I model the outcome of having voted. 

```{r}
SurvData_vot <- SurvData %>% 
  filter(!is.na(voted)) %>%  ## Filter out all obs that did not vote
  mutate(vot_binary = ifelse(voted == 1, 1, 0))

logit_m_1 <- glm(vot_binary ~ gender_cased + country + wave, family = "binomial", data = SurvData_vot)
logit_m_2 <- glm(vot_binary ~ gender_cased + country + wave + whostility, family = "binomial", data = SurvData_vot)
logit_m_3 <- glm(vot_binary ~ gender_cased + country + wave + whostility + interest, family = "binomial", data = SurvData_vot)

huxtable::huxreg(logit_m_1, logit_m_2, logit_m_3) ## This is where I would normally use stargazer 
                          ## But stargazer does not work in quarto html outputs
                          
```



# Plotting

To paraphrase the Beatles, all you need is ggplot. This framework allows you to plot essentially everything, although some packages obviously makes this plotting simpler, but most of them are just wrappers for ggplot code (even some python and Julia packages use ggplot in R as a backend). You could write entire books worth of information on plotting with ggplot. In fact someone did, I have it if you're interested. 

![neat book](./images/healy_vis.jpeg)

## Layers

It is prudent to think of visualisations in ggplot as layered. These layers are bound together with plus signs, and each construct a layer ontop of the plot or other visualisation you want to make. 

![layers of ggplot, stolen from: https://lfoswald.github.io/2021-spring-stats2/materials/session-3/03-online-tutorial/](./images/gglayers.png)

The most basic general layer that all ggplots need is the base layer of the `ggplot` function itself. In this function you specify your dataset, and the general aesthetics. Within the `ggplot` function (and most `geom_` derivatives) there is an `aes` argument, this is an aesthetic argument, it specifies what variable from the dataset should represent what element of the plot. Based on the plot type the type of aesthetic accepted varies, but a list of potential aesthetics are: x, y, color, fill, shape, size, linewidth, linetype, etc. Following this, you add a layer that specifies what type of plot you want, this is done through a family of `geom_` functions such as `geom_point` for scatterplot, `geom_bar` for a barplot, `geom_boxplot` for a boxplot with distributions, etc. Which you use depends on how you want to visualise. If you are curious about the options, type `geom_` in R, and switch through the optins that it auto-completes to. Lets use this information to build a scatterplot of the first grouped dataframe created earlier.

```{r}
grouped_f %>% ### Obviously you can pipe it in 
  ggplot(aes(x=year, y=whostility, color = country)) + 
  geom_point()

```
 Very ugly, but it does the job. We can now add a couple of layers to improve the look. There are several themes that can be used by default, such as `theme_bw` which is my favourite, just add the function in a layer. Additionally we can modify the labels of the plot through the `lab` function as a separate layer. 
 
```{r}
grouped_f %>% 
  ggplot(aes(x=year, y=whostility, color = country)) + 
  geom_point() + 
  theme_bw() + 
  labs(title = "Here's a Title", x = "Time period", y = "Anti-women sentiment", color = "Region")
```
 For the final plot, I will also do some changes to the datastructure so that it visualises a bit better, there's perhaps too much space between the timings as it is. Additionally, I think the points are too small, so I will change them manually through arguments fed through the layers. Finally, I also add a `geom_line` which when combined in this instance, draws a line between the points, separated by the color specification, by default, `geom_` functions inherit their aesthetic from the layer previous, so the same `aes` specification is inherited from the higher level. 
 
```{r}
grouped_f %>% 
  mutate(wave = case_when(             ### Mutating to better cases, so they are parallel
    year == 1995 | year == 1996 ~ 1,
    year == 2000 | year == 2001 ~ 2,
    year == 2005 ~ 3,
    year == 2010 ~ 4
  )) %>% 
  ggplot(aes(x=wave, y = whostility, color = country)) +  ### Setting aesthetic
  geom_point(size = 2) +  # Scatterplot, with 2 as size of the dots
  geom_line() +          # make a line, inherits past aesthetics
  theme_bw() +        # theme_bw is the best
  scale_color_manual(values = c("cornflowerblue", "goldenrod")) +  ## Values are assigned alphabetically when set manually, so Japan gets first colour
  labs(title = "Hostility towards women as legislators", x = "WVS Wave", y = "Hostility") ## Creating labels
```
The values and precise aesthetics of a variable can be set with the function family `scale_AESTHETIC_TYPE`, which provides a large range of ways to modify the aesthetics. In the plot above I manually (therefore manual at the end) assign two values to the color aesthetic (therefore color in the middle). This function family can also modify labels and other elements. 

We can also save the resulting plot, by assigning it to an object, and using the dedicated ggsave function, which allows for a ton of useful settings. 

```{r}
plot_obj <- grouped_f %>% 
  mutate(wave = case_when(             ### Mutating to better cases, so they are parallel
    year == 1995 | year == 1996 ~ 1,
    year == 2000 | year == 2001 ~ 2,
    year == 2005 ~ 3,
    year == 2010 ~ 4
  )) %>% 
  ggplot(aes(x=wave, y = whostility, color = country)) +  ### Setting aesthetic
  geom_point(size = 2) +  # Scatterplot, with 2 as size of the dots
  geom_line() +          # make a line, inherits past aesthetics
  theme_bw() +        # theme_bw is the best
  scale_color_manual(values = c("cornflowerblue", "goldenrod")) +  ## Values are assigned alphabetically when set manually, so Japan gets first colour
  labs(title = "Hostility towards women as legislators", x = "WVS Wave", y = "Hostility") ## Creating labels

ggsave("plot_obj.jpeg", plot = plot_obj, device = "jpeg", path = "./images",
       width = 12.5, height = 7.5)

## and now its here, contrast with earlier listes files in images

list.files("./images/")

unlink("./images/plot_obj.jpeg") ## Just removing it 

```


## Maps

You can also utilize the ggplot package, in conjunction with the sf package, to visualise maps. This requires shapefiles, which describes the general outlines of some countries. These can also be accessed through several packages, or downloaded from gadm manually. There are a lot of intricacies involving maps and shapefiles in general, but to use it with ggplot it is most common to have it as a `sf` type file, you can see that the map assignment in the code chunk below is piped into a `st_as_sf` function, this transform the `tera` (I believe it is) class object, into an `sf` class object. In some instances we would have to assign a crs as well, but that's perhaps too much complicated to talk about right now. Note also that the `ggplot` layer is itself empty, because the data and aesthetic mapping needs to be from a shapefile function of `geom_sf`. 
```{r}
library(geodata) # for gadm download
library(sf)

nor_map <- gadm(country = "NO", level = 1, path = "./") %>% 
  st_as_sf()

ggplot() +  ## in map data the ggplot is empty
  geom_sf(data = nor_map) + 
  theme_void() # common theme to eliminate all lines

```
Aesthetics are possible, filling the map to make a cloropleth for instance. You can also left join data that are not of a GIS character straight into this dataframe, then use those variables as aesthetics, but for now lets just make a new variable based on whether I have visited the county.

```{r}
nor_map <- nor_map %>% 
  mutate(me_there = case_when(
    HASC_1 == "NO.AA" ~ "Yes",
    HASC_1 == "NO.AK" ~ "Home",
    HASC_1 == "NO.BU" ~ "Yes",
    HASC_1 == "NO.FI" ~ "No",
    HASC_1 == "NO.HE" ~ "Yes",
    HASC_1 == "NO.HO" ~ "Yes",
    HASC_1 == "NO.MR" ~ "No",
    HASC_1 == "NO.NO" ~ "No",
    HASC_1 == "NO.NT" ~ "No",
    HASC_1 == "NO.OF" ~ "Yes",
    HASC_1 == "NO.OP" ~ "Yes",
    HASC_1 == "NO.OS" ~ "Yes",
    HASC_1 == "NO.RO" ~ "Yes",
    HASC_1 == "NO.SF" ~ "Home",
    HASC_1 == "NO.ST" ~ "Yes",
    HASC_1 == "NO.TE" ~ "Yes",
    HASC_1 == "NO.TR" ~ "No",
    HASC_1 == "NO.VA" ~ "Yes",
    HASC_1 == "NO.VF" ~ "Yes"
))

ggplot() + 
  geom_sf(data = nor_map, aes(fill = me_there)) + 
  theme_void() + 
  labs(title = "Mitt lille land", fill = "Have I been?") + 
  scale_fill_manual(values = c("goldenrod", "firebrick", "cornflowerblue"))
```
Note that the aesthetic argument is `fill` not `colour` (I have done that mistake several times). 



# Advanced methods

## For loops and flow control

A central part of programming in most languages is the flow control and for loops. For loops in R are slower than most other languages, and usually apply family of functions is preferred, since they can basically function in the same way as for loops, with the right specifications. But knowledge of loops can give you a tool to solve certain difficult tasks where you do not want to spend a couple of hours finding a relevant function. For loops consist of the function `for(iterator in sequence)`, the iterator is the thing that changes with each loop. A for loop is an instruction to do the same task over a set of times. 

Below is a simple for loop. First I create a vector of some numbers. The loop itself, I call the iterator i, and specify that I am looping over a sequence of numbers, from 1 to the length of the vector. I then use that operator as an index to the vector, so that for each iteration of the loop it prints the i vector position squared. 

```{r}

loopy_vector <- c(15,23,34,66,32)

for(i in 1:length(loopy_vector)){
  
  print(loopy_vector[i]^2) ### Note that unlike outside the loop, it does not print results automatically

}


```
For loops have what we call a global scope by default, meaning they can and will interact with global variables, and assignment inside the loops are tracked in the global environment, and change variables you have in your environment, unlike lapply, which usually only changes return values. 

### flow control

For loops are just one version of repeating code, `while` and `repeat` also do similar things, however, I have never used them, and highly doubt the reader will do as well. 

Flow control refers to ways in which you control a for loop or other similar iterative processes. These keywords are highlighted differently than other keyterms (in my instance they are red (I think, I cant see colours), but it varies with RStudio theme). These words are amonst others:

- `if(){}` code in the bracket only executed if the logical test within parenthesis returns TRUE, can be followed by else
- `else{}` follows the end of the if code bracket, code is executed if the test in the if function is returned as FALSE
- `next`, usually nested within an if or else test, skips to the next iteration of the loop. 
- `break`, exits the loop completely, stopping the iteration over loops, *needs* to be present in a repeat loop, or it will never stop
- `return`, specifies what object or element within the function or loop that should be returned when the iteration is complete, by default the function will return the last object created within it.  
- `switch`, is also base highlighted, and is in essence just a baseR version of case_when, but a fair bit more complicated, no need to learn it
- `stop`, nested within a statement of sorts, stops a function or loop, and executes an error action, for instance to print out where the error was thrown (oh my god why haven't I used this more for myself)
- `try`, attempts to execute, and will not stop a loop even if failing to execute the code. Can also be modified with error handling programming, and the `tryCatch` function, which can attempt to catch these errors. Useful for loops where you know certain elements will throw an error.

```{r}
## showing ifs and elses

numbers <- c(1:10)

for(i in 1:length(numbers)){
  
  if(numbers[i] %% 2 == 0){
    
    print(paste0("This number: ", numbers[i], ", is even!"))
    
  }else{
    print(paste0("This number: ", numbers[i], ", is odd!"))
  }
  
}


## Showing repeat, and break

iter <- 1 

repeat{
  
  print(paste0("iterator is now: ", iter, "!"))
  
  iter <- iter + 1
  
  if(iter == 12){
    
    print(paste0("oh no, iterator ", iter, " broke the loop! :("))
    
    break
    
  }
  
}


```


## Functional programming with anonymous functions

If you are uncertain about how to do certain data wrangling processes, it is always possible to write your own function, or own generic procedure for a single problem. Using the `lapply` function, or the general family of `apply` functions, one can assign a very efficient function to be applied over a set of elements in a list. The letter in front usually specifies what type of data you will get back, where `lapply` is the easiest to use, since it just returns a list, is therefore less likely to throw a merger error. `sapply` will attempt to return the simplest datatype possible, `vapply` will return a vector. Note that these types of anonymous functions have a different scope than for loops. All variables within the function exists solely within the function and cannot be called into the global environment. As previously mentioned this can be circumvented with the global environment assignment operator `<<-` but there is rarely a situation where the usage is necessary.  


```{r}

lapply(loopy_vector, FUN = function(x){ ## x serves here as our "iterator"
  
  y <- x^2
  
  return(y)
  
})


```
Note that the output from the function is a list, with nested singular values, a normal method is to wrap a `lapply` function in an `unlist` function, or to feed the `lapply` to one using a pipe. 

I demonstrate a more complex code chunk below, I will create a function that estimates Pi based on Monte Carlo sampling. Note the usage of local variables assigned, as well as the different usages of the iterator. Given that we are "looping" over the vector of different number, the iterator in each sequence becomes the number from the vector. 

```{r}

sample_nums <- c(10, 100, 1000, 10000, 100000)


lapply(sample_nums, function(x){
  
  x_coord <- runif(x, min = 0, max = 1)    ## Randomly create coordinates in a 1x1 square
  y_coord <- runif(x, min = 0, max = 1)    ## Note i use the x iterator as numbers of samples
  
  dist <- sqrt(x_coord^2 + y_coord^2) ## Measure their distance to origin
  withiners <- ifelse(dist <= 1, TRUE, FALSE)   ## If they have 1 or less in dist, they are inside the circle
  
  pi <- (sum(withiners)*4)/x ## the share of dots inside the circle multiplied by 4 is pi
  
  
  key <- paste0("Using ", x, " Samples, Pi is estimated to be ", pi, " (ish)") ## Create a return message
  
  return(key)
  
}) %>% unlist() ## Unlisting the results


```
For a more practical point, I will demonstrate using a lapply function to create a new variable in our old dataset. Let's say I want to construct an "anti-establishment" index, which is a function of opposition to EU or EEA, opposition to public services, opposition to gender equality, and extreme position on the left_right scale. In this instance I will utilize an `apply` function which will go over the rows of the dataset, in only the columns I want. When specifying a margin for the simple `apply` function, it will take either the rows or columns specified and turn them into vectors, so note that the iterator below can be indexed over, to get the relevant variable.

```{r}
vars_ext <- c("eu", "eea", "off_tjen", "likestilling", "left_right")


joined_sets2$anti_establishment <- apply(joined_sets2[, vars_ext], MARGIN = 1, ## Margin argument specifies to view each extract by row or column
                                         FUN = function(x){
                                           
                                           opposed_eu <- ifelse(x[1] > 4, 1, 0) ## If answered 5, completely disagree, then val 1
                                           
                                           opposed_eea <- ifelse(x[2] < 2, 1, 0) # This one is framed reverse
                                           
                                           opposed_public <- ifelse(x[3] > 4, 1, 0)
                                           
                                           opposed_equal <- ifelse(x[4] > 4, 1, 0)
                                           
                                           extreme <- ifelse(x[5] > 7 | x[5] < 3, 1, 0)
                                           
                                           anti_val <- mean(c(opposed_eu, opposed_eea, opposed_public, # index is the mean of these values
                                                            opposed_equal, extreme), na.rm = TRUE)
                                           
                                           return(anti_val)
                                           
                                         })


table(joined_sets2$anti_establishment)

```
Obviously more complicated operations can be done, but essentially this is here to demonstrate how you take several columns input to make a new variable, a process which can get tricky and complicated within a simple dplyr `mutate` function. 

If you know what you unit you want to do the same thing over and over again for, you do practically anything with an `apply` function. A good idea is to practice situations with it, because then when you are faced with an unfamiliar problem, you can go straight to work fixing the issue, rather than searching for obscure packages with functions that do the thing you want. 

### A very complicated for loop

Below is an example of a working loop that would take text information about zambian mp's and turn them into "spells" or period rows, where each row is an mp's time in a particular role. There is a heavy use of regular expressions, but the point is to demonstrate how one can go from a brick of text, into a dataset. Try returning to it at some later points, to see how your understanding of the process has improved. The dataset has simply three relevant variables, start, stop, and portfolio. Start is the starting date of the MP's period in parliament, and stop is the end of their time in this period of parliament. Meanwhile, portfolio is a long text that summarises their career, including roles in government. These roles were described as "... minister of local government (1976-1983)". The goal of the loop is to then split this info, into rows where each row is one such period, so the previously mentioned portfolio becomes one row for the time the MP spent as a minister of local government, then more rows for the other time periods. 

```{r}
#| eval: false


mps2 <- mps %>% 
  mutate(start = as.character(start),
         stop = ifelse(stop == "Active", "2023-01-01", stop)) ## Turns the start and stop to characters, and replaces "ACTIVE" with current year

for(i in 1:nrow(mps2)){
  
  if(is.na(mps2$portfolio[i])){ ## If it has no portfolio, then assume the entire period is just an mp, put it back in the list
    mps_list[[i]] <- mps2[i,]
    next
  }
  
  if(!str_detect(mps2$portfolio[i], "\\(\\d+(?:\\-\\d+)?\\)")){ # If the portfolio has no (year_start - year_end) parts, then go next
    mps_list[[i]] <- mps2[i,]
    next
  }else{
    
    tm1 <- str_extract(mps2$portfolio[i], "\\d+") ## First 4 digit number
    
    tm2 <- str_extract_all(mps2$portfolio[i], "\\d+")[[1]][2] ## Second 4 digit number
    
    spells_info_counter <- str_count(mps2$portfolio[i], "\\(\\d+(?:\\-\\d+)?\\)") # counting the amount of roles the mp has had
    
    t_start <- str_extract(mps2$start[i], "\\d{4}") # extracting the total period start
    
    t_end <- str_extract(mps2$stop[i], "\\d{4}") # and end
    
    if(tm1 == t_start & tm2 == t_end & !(is.na(tm2))){ # if they are equal, then the role covers the entire period
      mps_list[[i]] <- mps2[i,] # return to the list, go next
      next
    }else{
      
      spell_list <- list() # create an empty list to put all the roles into
      
      for(j in 1:spells_info_counter){ # for every time it found a role 
        
        curr_span <- as.numeric(unlist(str_extract_all(str_extract_all(mps$portfolio[i], "\\(\\d+(?:\\-\\d+)?\\)")[[1]][j], "\\d{4}"))) # get that exact role span
        
        if(all(curr_span > t_end) | all(curr_span < t_start)){ # if it is outside the range of the current row, ignore, go next in the role list
          next
        }
        
        if((max(curr_span) == t_start | min(curr_span) == t_end) & length(curr_span) != 1){
          next
        }
        
        
        spell_list[[j]]  <- mps2[i,] %>% 
          mutate(role = str_split(portfolio, "\\(\\d{4}(?:\\-\\d{4})?\\)")[[1]][j], ## Find the role, string of everything before the time period identifier
                 role_span = str_extract_all(portfolio, "\\(\\d+(?:\\-\\d+)?\\)")[[1]][j], # find the span of the role, the (start - stop) string
                 start = ifelse(str_extract(role_span, "\\d{4}") <= t_start, start, # if the start was before this row, then the rowstart is the start
                                str_extract(role_span, "\\d{4}")), # else the actual first part of the (start-stop) string
                 stop  = ifelse(str_count(role_span, "\\d+") == 1, str_extract(role_span, "\\d+"), # same with stop, but for the end
                                ifelse(str_extract_all(role_span, "\\d+")[[1]][2] >= t_end,
                                       stop,
                                       str_extract_all(role_span, "\\d+")[[1]][2])),
                 start = as.Date(ifelse(is.numeric(start), start,
                                        as.Date(paste0(start, "-1-1"), format = "%Y-%m-%d")), # turn it in to the Date format
                                 origin = "1970-01-01"),
                 stop = as.Date(ifelse(is.numeric(stop), stop,
                                       as.Date(paste0(stop, "-12-31"), format = "%Y-%m-%d")),
                                origin = "1970-01-01"),
                 start = as.character(start), # turn that into text again, so we have exact dates to work with
                 stop = as.character(stop)
          )
        
      }
      
      if(is_empty(spell_list)){ # if it for some reason returned empty, cause no periods to loop over, they were just an elected member in this period
        mps_list[[i]] <- mps2[i,] %>% 
          mutate(role = "Elected Member")
        next
      }
      
      speller_list <- bind_rows(spell_list) # turn the roles into a list
      
      Y_Periods <- speller_list %>% 
        mutate(start = as.Date(start, format = "%Y-%m-%d"),
               stop = as.Date(stop, format = "%Y-%m-%d")) %>%  ## Back into dates again
        mutate(int_int = interval(start,stop)) %>%  # create an interval between the start and the stop
        pull(int_int)
      
      X_Period <- mps2[i,] %>% 
        mutate(start = as.Date(start, format = "%Y-%m-%d"),
               stop = as.Date(stop, format = "%Y-%m-%d")) %>%  ## Create a frame with the total interval for the row
        mutate(int_int = interval(start,stop)) %>% 
        pull(int_int)
      
      seq_X <- as.Date(seq(int_start(X_Period), int_end(X_Period), by = "1 day"))
      seq_Y <- as.Date(do.call("c", lapply(Y_Periods, function(x)
        seq(int_start(x), int_end(x), by = "1 day"))))
      
      unique_dates_X <- seq_X[!seq_X %in% seq_Y] ## find the sequence of time not covered by these roles
      
      if(is_empty(unique_dates_X)){ # if there are no times they have no role, go next
        mps_list[[i]] <- speller_list
        next
      }else {
        
        lst <- aggregate( ### Create an aggregate of the timeholes
          unique_dates_X,
          by = list(cumsum(c(0, diff.Date(unique_dates_X) != 1))),
          FUN = function(x) c(min(x), max(x)),
          simplify = F)$x    
        
        my_ints <- lapply(lst, function(x) interval(x[1], x[2])) ## Create a new interval of this
        
        l_fram <- data.frame(inter_start = sapply(lapply(my_ints, FUN = int_start), as.character),
                             inter_end = sapply(lapply(my_ints, FUN = int_end), as.character))
        
        new_adds <- cbind(mps2[i,], l_fram) %>%  ## Pull a copy of the original row for personal info
          mutate(start = inter_start, stop = inter_end, # mutat to end and start from the interval we created.
                 role = "Elected Member") ## The empty times is when they were just an elected member
        
        mps_list[[i]] <- speller_list %>% ## Add it to the total list
          bind_rows(new_adds)
      }
    }
    
  }
}


```


## String manipulation

I will not go into details in this section, but will make it clear that character variables can be utilized in many different ways through what is known as regular expressions or regex, which are lines of very silly text which can be used as pattern matching to strings as data. 

```{r}
this_is_a_string <- "hi my name is Gard and I have clinical depression, please help"

## Regex looks insane, so when i for instance want to check whether there is a capitalized name in this string

library(stringr)

## Now i check the string on a pattern that matches to any substring that 
## starts with a single capitalized letter followed by 2 or more uncapitalized
## letters.

str_detect(this_is_a_string, "[A-Z]{1}[a-z]{2,}")

## Other functions could do similar stuff, like extract the match

str_extract(this_is_a_string, "[A-Z]{1}[a-z]{2,}")


## Same but for a comma and then anything

str_extract(this_is_a_string, "\\,.*")

## Anything following a comma, but not the comma

str_extract(this_is_a_string, "(?<=\\,).*") 

# Note the space is still there

```
Regex can get extremely complex, and string manipulation in general is a very advanced topic in terms of common R usage, I still have to google many of the characters used in these patterns, and to teach away how regex works properly could probably be an entire course or two in and of itself. 

## Keyboard shortcuts and tips

- ctrl + shift + m gives a pipe %>% 
- ctrl + shift + n creates a new script, useful for drafting purposes (sometimes)
- alt + - gives an assignment operator <- 
- shift + any arrow key, highlights text as you move the text entry thing
- ctrl + shift + left/right highlights while skipping elements, so you can highlight entire words
- If you have an entire section highlighted, you can wrap it in ", (, [, etc. By just pressing that button while having the thing highlighted
- ctrl + alt + up/down, gives you more cursors, so you can add more than one line at a time. Useful for repeated entries of the same thing

There is an extensive style guide, but generally R custom is to use what is called snake case for variable and object naming. The name "SurvData" used earlier violates this by being pascalcase, a strict snake case version would be "surv_data". point being, avoid upper case letters, bind words with an underscore. Same custom goes for filenaming of R-scripts and datasets. 

## Structuring your scripts

Scripts are there for reproduction, you could technically run your entire code through the console, but scripts exist to ensure that you can reproduce your results, and that others can reproduce your results. For that reason it is a good idea to have a good structure to your scripts. The main elements that you should always keep in mind is: Linearity, outline, readability, and comments. Linearity in this instance refers to a technical requirement as much as a readers requirement. It is common when beginning to write longer scripts, that you start to run elements of the code out of order. That is fine in and of itself, but becomes a problem when you start writing new code in places it does not make sense to, and when code that depends on variables specified later in your script is added. R is almost always run sequentially, if I am reviewing someone else's script, I will just highlight the entire script, and run all of it. You should ensure that your scripts will run on the first time they are done this way, therefore, be mindful of where your variable specifications are, and of where you change variables, where you create them, and when your output needs them to be used. 

Outline in this instance refers to the idea that you separate your script into easily identifiable chunks. This can actually be done in the comments. If you comment 4 or more #, followed by text and then 4 or more # or -, it will create a section in your script, which can be visible from the outline command.

![outline](./images/outliners.png)

You can then use this table of contents to more easily navigate the structure of your script. Additionally, creating such outlines is good to ensure the previously mentioned requirement of linearity.

When it comes to readability, it is important that you take a couple of key steps to ensure that your code is light on the eyes and the code viewer. Remember to have proper spacing between elements of your code, below are a couple of examples of good and bad practice when it comes to writing code with spacing. Note that you should also actively use linespacing to create blocks or separate code from each other. 

```{r}
df_example2<-df_example%>%filter(happy==FALSE) ## BAD
df_example_2 <- df_example %>% filter(happy == FALSE) ## Good

## Note the spacing between opeerators, but not between parenthesis and text. 

df_example2 <- df_example %>% filter(happy == FALSE,age == 27) ## BAD
df_example_2 <- df_example %>% filter(happy == FALSE, age == 27) ## GOOD

## Note that spacing after the commas, just like normal writing, is good practice

```

Additionally, your code should not be too wide. Perhaps you have noticed a strange grey line running down your script editor. This is the recommended width by posit, the company who makes RStudio. while it is generally fine to break this line once or twice, it should serve as a guide to how wide your script should be, to ensure that it is easy to follow. Linebreaks are automatically spaced in a nice way when you use them inside functions between arguments, as is evident from some of the code demonstrated here. Below is also an example of really bad and embarassing code from my MA thesis. 

![really bad stuff](./images/bad_stuff.png)

Note the absolute hell that is trying to find out if you have enough parentheses at the end of your mutate function. 


Finally, it is good practice to comment your code regularly. A # will turn everything on the line that follows it into a comment. Make sure that you provide the reader with some context as to how you are proceeding. 

## Quarto - creating nice documents in RStudio with code

The document you are currently reading is made through the quarto framework, which is developed in RStudio. This framework allows for seamless creation of pdf, html, and other types of files, from your RStudio IDE. The outline files for these documents end in .qmd, and you can create a new one from the top corner of your IDE. 

![here](./images/heres_quarto.png)

When clicking new quarto document, you will arrive at a prompt to create one of several types of documents, the current one is an html document, but through quarto you can also create latex specified documents (I have a friend who wrote their Master thesis in RStudio, you can also use bibref through it). Quarto can also make presentations through beamer, revealjs and other engines for making presentations (even powerpoint can be made through RStudio). Following the creation of the file, you specify several settings in the YAML header, which is at the top, here you can specify settings that affect the entire document, and the way it is rendered. The YAML header for this document for instance looks like this: 

![yaml](./images/yaml_header.png)

In the above instance, the document is turned into an html file, which is the type of file which web browsers display, hence why this tutorial is viewable like this in a webpage. Note also that the output html has several subcategories that you can use to edit settings for the document output you want. You can change the `output` specification into a different type of document, to change what it is knitted into. Do however make sure that the code you are entering is not unique to the filetype you are applying. If you want a LateX style document you specify `output: pdf`, this requires a LateX implementation, and the TinyTex package to be on your computer (no overleaf here). If you want a word document, you can specify `output: docx` in the header, and you can make further specifications by indenting it in new lines, such as in the example below, where a reference document which has settings for font types and sizes, is included in addition to the normal output.

![example yaml](./images/yaml_example_word.png)

Every type of output from Quarto has its own unique variations, that you can explore if you choose to go further with them. Here are some helpful pages for each of the main types of outputs. In addition to the normal outputs, you can also create presentations through beamer (a LateX implementation) and revealjs through Quarto. 

- Pdf: https://quarto.org/docs/output-formats/pdf-basics.html 
- html: https://quarto.org/docs/output-formats/html-basics.html
- word: https://quarto.org/docs/output-formats/ms-word.html
- Beamer Presentations: https://quarto.org/docs/presentations/beamer.html
- revaljs Presentations:  https://quarto.org/docs/presentations/revealjs/